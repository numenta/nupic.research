# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = results

# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://sahmad/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = True

experiment = grid

# Training
weight_decay = 0.0005

# Common network parameters
input_shape = (3, 32, 32)
boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.5

# Three layer specific parameters:
iterations = 200
repetitions = 5
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 128
test_batches_in_epoch = 500
learning_rate = 0.1
momentum = 0.5
learning_rate_gamma = 0.97

network_type = vgg
block_sizes = [1, 1, 1]
cnn_percent_on = [0.1, 0.1, 1.0]
cnn_weight_sparsity = [1.0, 0.5, 1.0]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [10, 12, 14]
linear_n = [50, 60]
linear_percent_on = [0.2, 1.0]
weight_sparsity = [0.3, 1.0]

stop = {"stop": 1}
checkpoint_at_end = True


[quick]
iterations = 3
repetitions = 1
batch_size = 4
batches_in_epoch = 4
first_epoch_batch_size = 4
batches_in_first_epoch = 4
test_batch_size = 4
test_batches_in_epoch = 4
block_sizes = [2, 3, 1]


[decentSparse]
iterations = 150
repetitions = 1
block_sizes = [1,3,2]
cnn_percent_on = [0.25, 0.25, 0.25]
cnn_weight_sparsity = [1.0, 0.5, 0.8]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [64, 128, 192]
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]
model_filename = "decentSparse.pth"
stop = {}


[decentDense]
iterations = 100
repetitions = 1
block_sizes = [1, 3, 2]
cnn_percent_on = [1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [39, 78, 62]
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

# Don't stop early
stop = {}

model_filename = "decentDense.pth"
checkpoint_at_end = True

[decent2Linear]
iterations = 100
repetitions = 1
cnn_weight_sparsity = [ 0.45, 0.88, 0.74 ]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [ 125, 93, 103 ]
linear_n = [578, 500]
linear_percent_on = [0.3, 0.3]
weight_sparsity = [0.5, 0.5]


[blockSizes]
iterations = 200
repetitions = 6
block_sizes = tune.sample_from(lambda spec: [np.random.randint(1,3), np.random.randint(1,3), np.random.randint(1,3)])
cnn_weight_sparsity = [ 0.2, 0.2, 0.2 ]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [ 64, 128, 128 ]
linear_n = [550]
linear_percent_on = [0.2]
weight_sparsity = [0.3]


[layerSearchSparse]
iterations = 125
repetitions = 20
block_sizes = tune.sample_from(lambda spec: [np.random.randint(1,3), np.random.randint(1,4), np.random.randint(1,4)])
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(5,30)/100.0, np.random.randint(5,30)/100.0, np.random.randint(5,30)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(30,100)/100.0, np.random.randint(30,100)/100.0, np.random.randint(30,100)/100.0])
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,128), np.random.randint(32,128), np.random.randint(32,196)])
linear_n = [550]
linear_percent_on = [0.3]
weight_sparsity = [0.4]


[layerSearchDense2]
iterations = 100
repetitions = 20
block_sizes = tune.sample_from(lambda spec: [np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)])
cnn_percent_on = [1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,128), np.random.randint(32,128), np.random.randint(32,128)])
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

# Don't stop early
stop = {}

# Search again over range of sparsities to see if sparsity correlates with robustness
[layerSearchDense3]
iterations = 125
repetitions = 7
block_sizes = [1,3,2]
cnn_percent_on = [1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(40,80), np.random.randint(100,150), np.random.randint(130,200)])
linear_n = [450]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

# Don't stop early
stop = {}
scheduler = None



# Test impact of weight decay on noise
[layerSearchDenseDecay]
iterations = 100
repetitions = 6
weight_decay = tune.sample_from(lambda spec: [0.0005, 0.0001, 0.001, 0][np.random.randint(0,4)])
block_sizes = tune.sample_from(lambda spec: [np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)])
cnn_percent_on = [1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,128), np.random.randint(32,128), np.random.randint(32,128)])
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

# Don't stop early
stop = {}
checkpoint_at_end = True


# Focus a bit more on the linear layer
[layerSearchSparse2]
iterations = 125
repetitions = 40
block_sizes = tune.sample_from(lambda spec: [1, np.random.randint(1,4), np.random.randint(1,4)])
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,30)/100.0, np.random.randint(20,30)/100.0, np.random.randint(20,30)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [1.0, np.random.randint(50,100)/100.0, np.random.randint(50,100)/100.0])
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(50,128), np.random.randint(50,128), np.random.randint(50,196)])
linear_n = tune.sample_from(lambda spec: [np.random.randint(300,800), np.random.randint(300,800)])
linear_percent_on = [0.3, 0.3]
weight_sparsity = [0.4, 0.4]


# Focus a bit more on boost factors
[layerSearchSparse3]
iterations = 125
repetitions = 40
boost_strength_factor = tune.sample_from(lambda spec: np.random.randint(1,20)/100.0 + 0.8)
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,30)/100.0, np.random.randint(20,30)/100.0, np.random.randint(20,30)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [1.0, np.random.randint(50,100)/100.0, np.random.randint(50,100)/100.0])
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(50,128), np.random.randint(50,128), np.random.randint(50,196)])
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]


# Focus a bit more on boost factors
[layerSearchSparse4]
iterations = 125
repetitions = 40
boost_strength_factor = tune.sample_from(lambda spec: np.random.randint(1,20)/100.0 + 0.8)
boost_strength = tune.sample_from(lambda spec: np.random.randint(0,20)/10.0)
k_inference_factor = tune.sample_from(lambda spec: np.random.randint(0,6)/10.0 + 1.0)
block_sizes = [1,3,2]
cnn_percent_on = [0.25, 0.25, 0.25]
cnn_weight_sparsity = [1.0, 0.5, 0.8]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [64, 128, 192]
linear_n = [550]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

checkpoint_at_end = False

# Use best boost, etc. and search again
[layerSearchSparse5]
iterations = 150
repetitions = 50
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(80,100)/100.0, np.random.randint(30,100)/100.0, np.random.randint(50,100)/100.0])
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,80), np.random.randint(100,150), np.random.randint(100,200)])
linear_n = tune.sample_from(lambda spec: [np.random.randint(200,800), np.random.randint(200,800)])
linear_percent_on = [0.3, 0.3]
weight_sparsity = [0.4, 0.4]

# Use best boost, etc. and search again over kernel sizes
[layerSearchSparse6]
iterations = 150
repetitions = 50
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(80,100)/100.0, np.random.randint(30,100)/100.0, np.random.randint(50,100)/100.0])
cnn_kernel_size = tune.sample_from(lambda spec: [np.random.randint(0,2)*2+3, np.random.randint(0,2)*2+3, np.random.randint(0,2)*2+3])
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,80), np.random.randint(80,150), np.random.randint(80,200)])
linear_n = tune.sample_from(lambda spec: [np.random.randint(200,800), np.random.randint(200,800)])
linear_percent_on = [0.3, 0.3]
weight_sparsity = [0.4, 0.4]


# Use best boost, etc. and search again over kernel sizes with one hidden layer
[layerSearchSparse7]
iterations = 150
repetitions = 50
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0, np.random.randint(20,40)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(80,100)/100.0, np.random.randint(30,100)/100.0, np.random.randint(50,100)/100.0])
cnn_kernel_size = tune.sample_from(lambda spec: [np.random.randint(0,2)*2+3, np.random.randint(0,2)*2+3, np.random.randint(0,2)*2+3])
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,80), np.random.randint(80,150), np.random.randint(80,200)])
linear_n = tune.sample_from(lambda spec: [np.random.randint(200,800)])
linear_percent_on = [0.3]
weight_sparsity = [0.4]


# Search again over range of sparsities to see if sparsity correlates with robustness
[layerSearchSparse9]
iterations = 100
repetitions = 7
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,70)/100.0]*3)
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(30,100)/100.0]*3)
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(40,80), np.random.randint(100,150), np.random.randint(130,200)])
linear_n = [450]
linear_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,70)/100.0])
weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(30,100)/100.0])

# Don't stop early
stop = {}
scheduler = None

# As 9, but longer and with early stopping
[layerSearchSparse10]
iterations = 150
repetitions = 7
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,70)/100.0]*3)
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(30,100)/100.0]*3)
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(40,80), np.random.randint(100,150), np.random.randint(130,200)])
linear_n = [450]
linear_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,70)/100.0])
weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(30,100)/100.0])


# As 10, but non-sparse linear, and no early stopping
[layerSearchSparse11]
iterations = 150
repetitions = 6
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,80)/100.0]*3)
cnn_weight_sparsity = [1.0, 0.8, 0.8]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(40,80), np.random.randint(100,150), np.random.randint(130,200)])
linear_n = [450]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

# Don't stop early
stop = {}
scheduler = None

# Weight decay with sparse nets
[layerSearchSparseDecay]
iterations = 150
repetitions = 20
weight_decay = tune.sample_from(lambda spec: [0.0005, 0.0001, 0.001, 0][np.random.randint(0,4)])
k_inference_factor = 1.0
block_sizes = [1,3,2]
cnn_percent_on = [0.3, 0.3, 0.3]
cnn_weight_sparsity = [1.0, 0.3, 0.75]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,80), np.random.randint(80,150), np.random.randint(80,200)])
linear_n = [300]
linear_percent_on = [0.3]
weight_sparsity = [0.4]


[layerSearchSparse12]
iterations = 150
repetitions = 20
k_inference_factor = 1.0
block_sizes = [1,2,2]
cnn_percent_on = [0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(3,20)*50, np.random.randint(1,20)*50, np.random.randint(1,20)*50])
linear_n = [500]
linear_percent_on = [0.3]
weight_sparsity = [0.3]

checkpoint_at_end = False


[layerSearchSparse13]
iterations = 200
repetitions = 10
k_inference_factor = 1.0
block_sizes = [1,2,2]
cnn_percent_on = [0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(3,20)*50, np.random.randint(1,20)*50, np.random.randint(1,20)*50])
linear_n = [750]
linear_percent_on = [0.3]
weight_sparsity = [0.4]

checkpoint_at_end = False

[layerSearchSparse14]
iterations = 150
repetitions = 4
k_inference_factor = 1.0
block_sizes = [1,2,2]
cnn_percent_on = [0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(3,20)*50, np.random.randint(1,20)*50, np.random.randint(1,20)*50])
linear_n = [500]
linear_percent_on = [0.3]
weight_sparsity = [0.3]

checkpoint_at_end = False

[layerSearchSparse15]
iterations = 200
repetitions = 10
k_inference_factor = 1.0
block_sizes = [2,3,2]
cnn_percent_on = [0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(3,20)*50, np.random.randint(1,20)*50, np.random.randint(1,20)*50])
linear_n = [500]
linear_percent_on = [0.3]
weight_sparsity = [0.3]

checkpoint_at_end = False


# AWS test - compare to layerSearchSparse15
[layerSearchSparse16]
iterations = 2
repetitions = 2
k_inference_factor = 1.0
block_sizes = [1,1,1]
cnn_percent_on = [0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5]
cnn_kernel_size = [5, 5, 5]
cnn_out_channels = [50, 50, 50]
linear_n = [500]
linear_percent_on = [0.3]
weight_sparsity = [0.3]

checkpoint_at_end = True

[VGG16Test]
use_max_pooling = True
iterations = 150
repetitions = 1
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [0.2, 0.2, 0.3, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

[VGG16Test2]
use_max_pooling = True
iterations = 150
repetitions = 1
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [0.2, 0.2, 0.3, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 5, 5, 5]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}


# Test learning rates and gammas
# Learning rates from 0.01 to 0.1, gamma from 0.80 to 0.97
# Momentum from 0.0 to 0.9
# Decay either 0 or 0.0005
[VGG16Test3]
weight_decay = tune.grid_search([0.0, 0.0005])
learning_rate_gamma = tune.sample_from(lambda spec: np.random.randint(0,18)*0.01 + 0.8)
learning_rate = tune.sample_from(lambda spec: np.random.randint(1,11)*0.01)
momentum = tune.sample_from(lambda spec: np.random.randint(0,10)*0.1)
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 100
repetitions = 16
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 5, 5, 5]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}


# Like above, but with higher GPU percentage
# Test learning rates and gammas
# Learning rates from 0.01 to 0.1, gamma from 0.80 to 0.97
# Momentum from 0.0 to 0.9
# Decay either 0 or 0.0005
# Epoch time train went from 109 down to 45 secs, GPU usage is 100%
[VGG16Test4]
weight_decay = tune.grid_search([0.0, 0.0005])
learning_rate_gamma = tune.sample_from(lambda spec: np.random.randint(0,18)*0.01 + 0.8)
learning_rate = tune.sample_from(lambda spec: np.random.randint(1,11)*0.01)
momentum = tune.sample_from(lambda spec: np.random.randint(0,10)*0.1)
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 100
repetitions = 16
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 5, 5, 5]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}


[VGG16Test5]
weight_decay = 0.0005
learning_rate_gamma = 0.96
learning_rate = 0.02
momentum = 0.8
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 200
repetitions = 1
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}


[VGG19Test1]
weight_decay = 0.0005
learning_rate_gamma = 0.96
learning_rate = 0.02
momentum = 0.8
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 200
repetitions = 1
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}


[VGG19Test2]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = 0.9
batch_size = 128
batches_in_epoch = 400
first_epoch_batch_size = 128
batches_in_first_epoch = 400
use_max_pooling = True
iterations = 200
repetitions = 1
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}


[VGG19SparseTest2]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = tune.grid_search([0.5, 0.0])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 200
repetitions = 4
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}


[VGG19SparseTest3]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [45, 65, 75]
learning_rate = 0.1
momentum = tune.grid_search([0.5, 0.8])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 100
repetitions = 8
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(10,40)/100.0]*5)
;cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

[VGG19SparseTest4]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [45, 65, 95]
learning_rate = 0.1
momentum = tune.grid_search([0.8, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 120
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0]*5)
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

[VGG19SparseTest5]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [55, 75, 100]
learning_rate = 0.1
momentum = tune.grid_search([0.8, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 120
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0]*5)
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

# Try kernel size 5 - almost 4x slower! Results are not any better.
[VGG19SparseTest6]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [55, 75, 100]
learning_rate = 0.1
momentum = tune.grid_search([0.8, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 120
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0]*5)
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [5, 5, 5, 5, 5]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}


# Try a sparse linear layer
[VGG19SparseTest7]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [55, 80, 100]
learning_rate = 0.1
momentum = tune.grid_search([0.8, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 120
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0]*5)
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = [512]
linear_percent_on = [0.3]
weight_sparsity = [0.3]

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

# VGG19SparseTest4 and 2 with early stopping to get decent noise results
[VGG19SparseTest8]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = tune.grid_search([0.5, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 150
repetitions = 4
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(20,40)/100.0]*5)
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0


# Like VGG19SparseTest8 with early stopping to get decent noise results but try different sparsities
[VGG19SparseTest9]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = tune.grid_search([0.5, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 150
repetitions = 4
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(15,30)/100.0]*5)
;cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0


# Like VGG19SparseTest8 with more early stopping to get decent noise results but try different sparsities
[VGG19SparseTest10]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = tune.grid_search([0.5, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 150
repetitions = 4
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(15,30)/100.0]*5)
;cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0

# Like VGG19SparseTest8 with more early stopping to get decent noise results but with fixed sparsities
[VGG19SparseTest11]
weight_decay = 0.0005
learning_rate_gamma = 0.1
lr_step_schedule = [81, 122]
learning_rate = 0.1
momentum = 0.9
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 150
repetitions = 4
k_inference_factor = 1.0
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0

# This gets up to 89.5% accuracy
[VGG16Test6]
weight_decay = 0.0005
learning_rate_gamma = 0.96
learning_rate = 0.02
momentum = 0.8
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 200
repetitions = 0
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = [512]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}


# Test step scheduler
[VGG16Test7]
weight_decay = 0.0005
learning_rate_gamma = 0.1
learning_rate = 0.1
lr_step_schedule = [4, 8]
momentum = 0.9
batch_size = 128
batches_in_epoch = 250
use_max_pooling = True
iterations = 200
repetitions = 0
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 64, 256, 512, 512]
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = [512]
linear_percent_on = [1.0]
weight_sparsity = [1.0]

checkpoint_at_end = True
gpu_percentage = 0.5
stop = {}

# VGG 16 using a known learning rate schedule
[VGG16Test10]
weight_decay = 0.0005
learning_rate_gamma = 0.1
learning_rate = 0.1
lr_step_schedule = [81, 122]
momentum = tune.grid_search([0.5, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 175
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(15,30)/100.0]*5)
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}


# VGG 16 using a known learning rate schedule and fixed percent ONs.
[VGG16Test9]
weight_decay = 0.0005
learning_rate_gamma = 0.1
learning_rate = 0.1
lr_step_schedule = [81, 122]
momentum = tune.grid_search([0.5, 0.9])
batch_size = 128
batches_in_epoch = 400
use_max_pooling = True
iterations = 175
repetitions = 2
k_inference_factor = 1.0
block_sizes = [2,2,3,3,3]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_percent_on = cnn_percent_on = [0.2, 0.2, 0.2, 0.2, 0.3]
cnn_weight_sparsity = [1.0, 0.5, 0.5, 0.5, 0.5]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = []

checkpoint_at_end = True
gpu_percentage = 1.0
stop = {}

