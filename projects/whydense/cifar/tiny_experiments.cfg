# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = results
seed = 42

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = True

experiment = grid
repetitions = 1

# Training
iterations = 60
weight_decay = 0.0005

# Common network parameters
input_shape = (3, 32, 32)
cnn_out_channels = 32

boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.5

[quick]
iterations = 3
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [0.1, 0.1]
cnn_weight_sparsity = [0.5, 1.0]
cnn_kernel_size = [5, 3]
cnn_out_channels = [8, 8]
linear_n = 100
linear_k = 0.1
weight_sparsity = 0.3

checkpoint_at_end = True
model_filename = "quick.pt"
scheduler = None

# One layer sparse network
[tinySparse1]
iterations = 15
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.01, 0.05, 0.1])
momentum = tune.grid_search([0.0, 0.5, 0.9])
learning_rate_gamma = tune.grid_search([0.7, 0.8, 0.9, 0.95, 1.0])

network_type = tiny_sparse
cnn_k = [0.2]
cnn_out_channels = [8]
linear_n = 100
linear_k = 0.2
weight_sparsity = 0.4

stop = {"stop": 1}


# One layer sparse network
# Best were cnn_out_channels=64, n=200.
[tinySparse11]
iterations = 20
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([[0.3], [0.2], [0.1]])
cnn_out_channels = tune.grid_search([[8], [16], [32], [64]])
linear_n = tune.grid_search([100, 200, 400])
linear_k = tune.grid_search([0.1, 0.2])
weight_sparsity = 0.4

stop = {"stop": 1}

[tinySparse1Decent]
iterations = 20
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [0.2]
cnn_weight_sparsity = [1.0]
cnn_kernel_size = [3]
cnn_out_channels = [64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
model_filename = "tinySparse1Decent.pt"


# Two layer sparse network
[tinySparse2Search]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 2000
test_batch_size = 64
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([[0.2, 0.3], [0.2, 0.1], [0.1, 0.2]])
cnn_out_channels = tune.grid_search([[64, 32], [64, 64], [32, 32], [32, 64]])
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}


# Two layer sparse network
[tinySparse2]
iterations = 50
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 2000
test_batch_size = 64
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [0.2, 0.2]
cnn_out_channels = [64, 64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True
model_filename = "tinySparse2.pt"

[tinyDense]
iterations = 10
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [1.0, 1.0]
cnn_out_channels = [8, 8]
linear_n = 100
linear_k = 1.0
weight_sparsity = 1.0


# A one layer dense network
[dense1]
iterations = 10
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [1.0]
cnn_out_channels = [8]
linear_n = 100
linear_k = 1.0
weight_sparsity = 1.0

# Investigate various options for the first layer
[tinyOptionsL1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([ [0.2], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.5], [1.0] ])
cnn_kernel_size = tune.grid_search([ [3], [5] ])
cnn_out_channels = [64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True


# Investigate more options for the first layer
[tinyOptions2L1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([ [0.05], [0.1], [0.2], [0.3], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.3], [0.5], [1.0] ])
cnn_kernel_size = [5]
cnn_out_channels = [64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate more options for the first layer
[tinyOptions3L1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([ [0.05], [0.1], [0.2], [0.3], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.3], [0.5], [1.0] ])
cnn_kernel_size = [5]
cnn_out_channels = [96]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for the second layer
[tinyOptions1L2]
iterations = 40
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = tune.grid_search([ [0.2, 1.0], [0.2, 0.2], [0.2, 0.3],])
cnn_weight_sparsity = tune.grid_search([ [0.5, 0.5], [0.5, 1.0] ])
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for a dense two layer
[bestDense2Layer]
iterations = 40
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_k = [1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_k = 1.0
weight_sparsity = 1.0

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for a dense two layer
[dense2LayerOptions]
iterations = 60
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.05, 0.1, 0.025])
momentum = tune.grid_search([0.0, 0.25, 0.5, 0.75])
learning_rate_gamma = tune.grid_search([0.85, 0.9, 0.95])

network_type = tiny_sparse
cnn_k = [1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = tune.grid_search([[5, 5], [3, 3]])
cnn_out_channels = [64, 64]
linear_n = 200
linear_k = 1.0
weight_sparsity = 1.0

stop = {"stop": 1}
checkpoint_at_end = True

# Search learning rates
[tinyOptions2L2]
iterations = 60
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.05, 0.1, 0.025])
momentum = tune.grid_search([0.0, 0.25, 0.5, 0.75])
learning_rate_gamma = tune.grid_search([0.85, 0.9, 0.95])

network_type = tiny_sparse
cnn_k = [0.2, 0.2]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_k = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True
scheduler = None

