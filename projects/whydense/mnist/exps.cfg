# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

#
# A laundry list of many of the experiments we performed.
#

[DEFAULT]
path = ~/nta/results/mnist
verbose = 2
checkpoint_freq = 1
checkpoint_at_end = False

; Uncomment to save results to S3
;upload_dir = "s3://bucketname/ray/whydense/mnist"
;sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

; Uncomment to average over multiple seeds
;repetitions = 1
;seed = 42
repetitions = 10
seed = tune.sample_from(lambda spec: np.random.randint(1, 10000))

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = False

data_dir = ~/nta/data/
no_cuda = False
log_interval = 2000

# Common network parameters
weight_sparsity = (0.3, )
cnn_weight_sparsity = (1.0,)
use_batch_norm = False
boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.0

# Common training regime / optimizer parameters
iterations = 15
validation = 1.0
optimizer = SGD
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
dropout = 0.0
batches_in_epoch = 100000
batch_size = 64
first_epoch_batch_size = %(batch_size)s
batches_in_first_epoch = %(batches_in_epoch)s
test_noise_every_epoch = False
test_batch_size = 1000
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.0

[quick]
iterations = 3
cnn_out_channels = (4,)
cnn_percent_on = (0.1,)
cnn_weight_sparsity = (0.8,)
linear_n = (20,)
linear_percent_on = (0.3,)
boost_strength = 0.0
momentum = 0.9
weight_sparsity = (0.4, )
batches_in_epoch = 2
batches_in_first_epoch = %(batches_in_epoch)s
test_noise_every_epoch = False


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;


[denseCNN1]
cnn_out_channels = (30,)
cnn_percent_on = (1.0,)
cnn_weight_sparsity = (1.0,)
linear_n = (1000,)
linear_percent_on = (1.0,)
weight_sparsity = (1.0, )
boost_strength = 0.0
momentum = 0.9

[denseCNN2]
cnn_out_channels = (30, 30)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (1000,)
linear_percent_on = (1.0,)
boost_strength = 0.0
momentum = 0.9
weight_sparsity = (1.0, )

[sparseCNN1]
cnn_out_channels = (30,)
cnn_percent_on = (0.093,)
linear_n = (150,)
linear_percent_on = (0.333,)
weight_sparsity = (0.3, )
first_epoch_batch_size = 4


[sparseCNN2]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.087, 0.293)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (700,)
linear_percent_on = (0.143,)
weight_sparsity = (0.3, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4


[sparseCNN2WtSparsity1]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.087, 0.293)
cnn_weight_sparsity = (1.0, 0.3)
linear_n = (700,)
linear_percent_on = (0.143,)
weight_sparsity = (0.3, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.5

[sparseCNN2SparsityExperiments]
cnn_out_channels = (32, 64)
cnn_percent_on = tune.sample_from(lambda spec: [0.1, np.random.randint(10, 30)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(40, 100)/100.0, np.random.randint(10, 60)/100.0])
linear_n = (700,)
linear_percent_on = tune.sample_from(lambda spec: [np.random.randint(10, 30)/100.0])
weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(10, 40)/100.0])
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
repetitions = 30
k_inference_factor = 1.5

# Some good ones to try based on sparseCNN2SparsityExperiments
# cnn_percent_on=[0.1 0.18]cnn_weight_sparsity=[0.97 0.21]linear_percent_on=[0.29]weight_sparsity=[0.11]
# 98.69 ± 0.00 | 101,406 ±  0.00 | 92.19 ±  0.00    | 2777.13 ± 0.00  | 98,426
[sparseCNN2_1]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.2)
cnn_weight_sparsity = (1.0, 0.2)
linear_n = (700,)
linear_percent_on = (0.3, )
weight_sparsity = (0.11, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.5

# Some good ones to try based on sparseCNN2SparsityExperiments
# cnn_percent_on=[0.1 0.16]cnn_weight_sparsity=[0.77 0.48]linear_percent_on=[0.24]weight_sparsity=[0.3]
# 98.91 ± 0.00 | 103,500 ±  0.00 | 94.09 ±  0.00    | 2857.83 ± 0.00  | 247,890
[sparseCNN2_2]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.16)
cnn_weight_sparsity = (0.8, 0.5)
linear_n = (700,)
linear_percent_on = (0.25, )
weight_sparsity = (0.3, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.5

# Some good ones to try based on sparseCNN2SparsityExperiments
# cnn_percent_on=[0.1 0.25]cnn_weight_sparsity=[0.56 0.53]linear_percent_on=[0.29]weight_sparsity=[0.13]
# 98.84 ± 0.00 | 103,375 ±  0.00 | 93.98 ±  0.00    | 2982.67 ± 0.00  | 128,490
[sparseCNN2_3]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.25)
cnn_weight_sparsity = (0.5, 0.5)
linear_n = (700,)
linear_percent_on = (0.25, )
weight_sparsity = (0.15, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.5

# Some good ones to try based on sparseCNN2SparsityExperiments
# cnn_percent_on=[0.1 0.29]cnn_weight_sparsity=[0.97 0.56]linear_percent_on=[0.27]weight_sparsity=[0.12]
# 98.90 ± 0.00 | 102,472 ±  0.00 | 93.16 ±  0.00    | 3105.91 ± 0.00  | 123,346
[sparseCNN2_4]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.25)
cnn_weight_sparsity = (1.0, 0.5)
linear_n = (700,)
linear_percent_on = (0.25, )
weight_sparsity = (0.12, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.5

# This is just like sparseCNN2SparsityExperiments except that k-inference factor
# is set to 1.0
[sparseCNN2KInference]
cnn_out_channels = (32, 64)
cnn_percent_on = tune.sample_from(lambda spec: [0.1, np.random.randint(10, 30)/100.0])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(40, 100)/100.0, np.random.randint(10, 60)/100.0])
linear_n = (700,)
linear_percent_on = tune.sample_from(lambda spec: [np.random.randint(10, 30)/100.0])
weight_sparsity = tune.sample_from(lambda spec: [np.random.randint(10, 40)/100.0])
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.0
repetitions = 30


# Some good ones to try based on sparseCNN2KInference
# cnn_percent_on=[0.1 0.28]cnn_weight_sparsity=[0.53 0.56]linear_percent_on=[0.17]weight_sparsity=[0.25]
# 99.13 ± 0.00 | 105,697 ±  0.00 | 96.09 ±  0.00    | 2827.76 ± 0.00  | 216,094
[sparseCNN2_5]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.28)
cnn_weight_sparsity = (0.55, 0.55)
linear_n = (700,)
linear_percent_on = (0.17, )
weight_sparsity = (0.25, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.0

# Some good ones to try based on sparseCNN2KInference
# cnn_percent_on=[0.1 0.15]cnn_weight_sparsity=[0.89 0.23]linear_percent_on=[0.24]weight_sparsity=[0.12]
# 99.05 ± 0.00 | 105,310 ±  0.00 | 95.74 ±  0.00    | 2658.99 ± 0.00  | 106,386
[sparseCNN2_6]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.15)
cnn_weight_sparsity = (0.9, 0.25)
linear_n = (700,)
linear_percent_on = (0.24, )
weight_sparsity = (0.12, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.0

# Some good ones to try based on sparseCNN2KInference
# cnn_percent_on=[0.1 0.2]cnn_weight_sparsity=[0.62 0.45]linear_percent_on=[0.2]weight_sparsity=[0.19]
# 99.09 ± 0.00 | 105,048 ±  0.00 | 95.50 ±  0.00    | 2717.94 ± 0.00  | 167,826
[sparseCNN2_7]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.2)
cnn_weight_sparsity = (0.62, 0.45)
linear_n = (700,)
linear_percent_on = (0.2, )
weight_sparsity = (0.20, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.0

# Based on sparseCNN2_7, this will be new sparseCNN2 candidate
[sparseCNN2_8]
cnn_out_channels = (32, 64)
cnn_percent_on = (0.1, 0.2)
cnn_weight_sparsity = (0.6, 0.45)
linear_n = (700,)
linear_percent_on = (0.2, )
weight_sparsity = (0.20, )
learning_rate = 0.02
momentum = 0.0
first_epoch_batch_size = 4
k_inference_factor = 1.0

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; Exploring parameters for dense networks.

# With exact same numbers as sparse CNN 2
# This should become the new denseCNN2
[denseCNN2Same]
cnn_out_channels = (32, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (700,)
linear_percent_on = (1.0,)
weight_sparsity = (1.0, )
boost_strength = 0.0
momentum = 0.9


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; Experimentation for the mixed networks described in the MNIST section
; of the paper.

# Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
[denseCNN2SP3Exp]
cnn_out_channels = (32, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (700,)
linear_percent_on = (0.143,)
weight_sparsity = (0.2, )
first_epoch_batch_size = 4
momentum = tune.sample_from(lambda spec: np.random.randint(0, 10)/10.0)
learning_rate = tune.sample_from(lambda spec: np.random.randint(1, 10)/1000.0)

# Sparse CNN-2 with sparse weights but dense activations
[sparseCNN2DSWExp]
cnn_out_channels = (32, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (0.6, 0.45)
linear_n = (700,)
linear_percent_on = (1.0, )
weight_sparsity = (0.2, )
first_epoch_batch_size = 4
momentum = tune.sample_from(lambda spec: np.random.randint(0, 10)/10.0)
learning_rate = tune.sample_from(lambda spec: np.random.randint(1, 10)/1000.0)

