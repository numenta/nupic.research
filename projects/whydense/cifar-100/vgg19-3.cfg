
# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = "~/nta/results"

# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://lsouza/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = False
experiment = grid
checkpoint_at_end = True

# Including dataset and output size as part of config
output_size = 100
dataset = CIFAR100
data_dir = "~/nta/datasets"

# Common network parameters - based on NeurIPS paper
input_shape = (3, 32, 32)
use_max_pooling = True
network_type = vgg
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []
weight_sparsity = [1.0]

# execution
iterations = 164
gpu_percentage = 1.0
cpu_percentage = 1.0
stop = {"stop": 1}

# batch configurations
batch_size = 128
batches_in_epoch = 500
test_batch_size = 128
test_batches_in_epoch = 100

# optimizer
optimizer= SGD
momentum = 0.4923
learning_rate = 0.0884
learning_rate_gamma = 0.1146
lr_step_schedule = [81, 122]
weight_decay = 0.0007


#########################################################################
[C100_SparseBest_Linear1_T]
repetitions = 6

first_epoch_batch_size = 4
batches_in_first_epoch = 750

k_inference_factor = 0.99
boost_strength_factor = 0.6985
boost_strength = 1.4042
cnn_percent_on = [0.31, 0.31, 0.31, 0.31, 0.31, 0.31]
cnn_weight_sparsity = [1.0, 0.75, 0.75, 0.75, 0.75]

linear_n = [512]
linear_percent_on = [0.65]
weight_sparsity = tune.sample_from(lambda spec: [np.random.uniform(0.1, 0.9)])


#########################################################################
[C100_SparseBest_Linear2_T]
repetitions = 6

first_epoch_batch_size = 4
batches_in_first_epoch = 750

k_inference_factor = 0.99
boost_strength_factor = 0.6985
boost_strength = 1.4042
cnn_percent_on = [0.31, 0.31, 0.31, 0.31, 0.31, 0.31]
cnn_weight_sparsity = [1.0, 0.75, 0.75, 0.75, 0.75]

linear_n = [512, 256]
linear_percent_on = [0.65, 0.65]
weight_sparsity = tune.sample_from(lambda spec: [np.random.uniform(0.1, 0.9)]*2)

#########################################################################
[C100_CPU_Test]
repetitions = 1
iterations = 2
batches_in_first_epoch = 10
batches_in_epoch = 10
gpu_percentage = 0
cpu_percentage = 1.0

k_inference_factor = 0.99
boost_strength_factor = 0.6985
boost_strength = 1.4042
cnn_percent_on = [0.31, 0.31, 0.31, 0.31, 0.31, 0.31]
cnn_weight_sparsity = [1.0, 0.75, 0.75, 0.75, 0.75]

linear_n = []
linear_percent_on = []