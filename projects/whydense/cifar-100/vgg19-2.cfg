# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = "~/nta/results"

# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://lsouza/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = False
experiment = grid
checkpoint_at_end = True

# Including dataset and output size as part of config
output_size = 100
dataset = CIFAR100
data_dir = "~/nta/datasets"

# Common network parameters - based on NeurIPS paper
input_shape = (3, 32, 32)
use_max_pooling = True
network_type = vgg
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_kernel_size = [3, 3, 3, 3, 3]
linear_n = []
linear_percent_on = []

# defaults, might change in the experiment
weight_decay = tune.sample_from(lambda spec: np.random.uniform(0.0003,0.007))
boost_strength = 1.5
boost_strength_factor = 0.85

# execution
iterations = 200
gpu_percentage = 0.165 # with bs 128 can fit up to 6 batches in 1 GPU
cpu_percentage = 1.0 # 8 cores per GPU
stop = {"stop": 1}

# batch configurations
batch_size = 128
batches_in_epoch = 500
test_batch_size = 128
test_batches_in_epoch = 500


#########################################################################
[C100_DenseAdam]
repetitions = 10

first_epoch_batch_size = 128
batches_in_first_epoch = 500

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= Adam
learning_rate = tune.sample_from(lambda spec: np.random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]))

#########################################################################
[C100_DenseSGD]
repetitions = 10

first_epoch_batch_size = 128
batches_in_first_epoch = 500

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))

#########################################################################
[C100_SparseAdam]
repetitions = 30
first_epoch_batch_size = 4
batches_in_first_epoch = 750

# sparsity specific
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.25, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [1]+[np.random.uniform(0.4, 0.7)]*4)
boost_strength = tune.sample_from(lambda spec: np.random.uniform(1.2,2.0))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.6,0.9))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.88, 1.08))
weight_sparsity = []

# optimizer
optimizer= Adam
learning_rate = tune.sample_from(lambda spec: np.random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]))

#########################################################################
[C100_SparseSGD]
repetitions = 30
first_epoch_batch_size = 4
batches_in_first_epoch = 750

# sparsity specific
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.25, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [1]+[np.random.uniform(0.4, 0.7)]*4)
boost_strength = tune.sample_from(lambda spec: np.random.uniform(1.2,2.0))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.6,0.9))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.88, 1.08))
weight_sparsity = []

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))


#########################################################################
[C10_DenseAdam]
repetitions = 5

first_epoch_batch_size = 128
batches_in_first_epoch = 500

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= Adam
learning_rate = tune.sample_from(lambda spec: np.random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]))

# dataset
output_size = 10
dataset = CIFAR10

#########################################################################
[C10_DenseSGD]
repetitions = 5

first_epoch_batch_size = 128
batches_in_first_epoch = 500

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))

# dataset
output_size = 10
dataset = CIFAR10

#########################################################################
[C10_SparseAdam]
repetitions = 10
first_epoch_batch_size = 4
batches_in_first_epoch = 750

# sparsity specific
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.25, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [1]+[np.random.uniform(0.4, 0.7)]*4)
boost_strength = tune.sample_from(lambda spec: np.random.uniform(1.2,2.0))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.6,0.9))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.88, 1.08))
weight_sparsity = []

# optimizer
optimizer= Adam
learning_rate = tune.sample_from(lambda spec: np.random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]))

# dataset
output_size = 10
dataset = CIFAR10

#########################################################################
[C10_SparseSGD]
repetitions = 10
first_epoch_batch_size = 4
batches_in_first_epoch = 750

# sparsity specific
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.25, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [1]+[np.random.uniform(0.4, 0.7)]*4)
boost_strength = tune.sample_from(lambda spec: np.random.uniform(1.2,2.0))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.6,0.9))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.88, 1.08))
weight_sparsity = []

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))

# dataset
output_size = 10
dataset = CIFAR10


#########################################################################
[VGG19SparseTest]
repetitions = 1
first_epoch_batch_size = 4
batches_in_first_epoch = 10
batches_in_epoch = 10
iterations = 2

# sparsity specific
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.25, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [1]+[np.random.uniform(0.4, 0.7)]*4)
boost_strength = tune.sample_from(lambda spec: np.random.uniform(1.2,2.0))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.6,0.9))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.88, 1.08))
weight_sparsity = []

# optimizer
optimizer= Adam
learning_rate = tune.sample_from(lambda spec: np.random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]))

#########################################################################
[VGG19DenseTest]
repetitions = 1
first_epoch_batch_size = 4
batches_in_first_epoch = 10
batches_in_epoch = 10
iterations = 2

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))

#########################################################################
[TestCIFAR10]
repetitions = 1

first_epoch_batch_size = 128
batches_in_first_epoch = 10
batches_in_epoch = 10

k_inference_factor = 1.0
cnn_percent_on = [1.0, 1.0, 1.0, 1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0, 1.0, 1.0, 1.0]
weight_sparsity = [1.0]

# optimizer
optimizer= SGD
momentum = tune.sample_from(lambda spec: np.random.uniform(0.4,0.6))
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.06,0.12))
lr_step_schedule = True
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.08,0.10))

# dataset
output_size = 10
dataset = CIFAR10


#########################################################################
#########################################################################
[VGG19SparseFull]

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = False
experiment = grid
checkpoint_at_end = False

# Including dataset and output size as part of config
output_size = 100
dataset = CIFAR100

# Including fixed path to dataset
data_dir = "~/nta/datasets"

# Common network parameters - based on NeurIPS paper
input_shape = (3, 32, 32)
use_max_pooling = True
network_type = vgg
block_sizes = [2,2,4,4,4]
cnn_out_channels = [64, 128, 256, 512, 512]
cnn_kernel_size = [3, 3, 3, 3, 3]
lr_step_schedule = [81, 122] # same as bearpaw

# why are these off, what does it mean?
linear_n = []
linear_percent_on = []
weight_sparsity = []

# test parameters
test_batch_size = 128
test_batches_in_epoch = 500

# execution
iterations = 164 # same as bearpaw for fair comparison
gpu_percentage = 0.165 # with bs 128 can fit up to 6 batches in 1 GPU
cpu_percentage = 1.0 # 8 cores per GPU
repetitions= 150

# sparsity specific configuration - see why
first_epoch_batch_size = 4
batches_in_first_epoch = 600

## change all grid search to sample from
## grid search

# more general
learning_rate = tune.sample_from(lambda spec: np.random.uniform(0.05,0.15))
learning_rate_gamma = tune.sample_from(lambda spec: np.random.uniform(0.05,0.20))
weight_decay = tune.sample_from(lambda spec: np.random.uniform(0.0003,0.0012))
momentum = tune.sample_from(lambda spec: np.random.uniform(0.3,0.99))
batch_size = tune.grid_search([64, 128])
batches_in_epoch = tune.sample_from(lambda spec: int(np.random.uniform(300,600)))

# sparsity specific
boost_strength = tune.sample_from(lambda spec: np.random.uniform(0.8,1.8))
boost_strength_factor = tune.sample_from(lambda spec: np.random.uniform(0.5,1.0))
k_inference_factor = tune.sample_from(lambda spec: np.random.uniform(0.8, 1.2))
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.15, 0.35)]*5)
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.uniform(0.2, 1)]*5)



