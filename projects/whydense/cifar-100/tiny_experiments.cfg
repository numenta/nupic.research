# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
path = results
seed = 42

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = True

experiment = grid
repetitions = 1

# Training
iterations = 60
weight_decay = 0.0005

# Common network parameters
input_shape = (3, 32, 32)
cnn_out_channels = 32

boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.0

[quick]
iterations = 3
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = vgg
cnn_percent_on = [0.1, 0.1, 0.1]
cnn_weight_sparsity = [0.5, 1.0, 0.3]
cnn_kernel_size = [5, 3, 5]
cnn_out_channels = [8, 8, 8]
linear_n = 100
linear_percent_on = 0.1
weight_sparsity = 0.3

checkpoint_at_end = True
model_filename = "quick.pth"
scheduler = None

[quickTmp]
iterations = 3
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = 0.01
momentum = tune.grid_search([0.0, 0.5, 0.9])
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [0.1, 0.1]
cnn_weight_sparsity = [0.5, 1.0]
cnn_kernel_size = [5, 3]
cnn_out_channels = [8, 8]
linear_n = 100
linear_percent_on = 0.1
weight_sparsity = 0.3

checkpoint_at_end = True
model_filename = "quick.pth"
scheduler = None

[quickTmp1]
iterations = 20
repetitions = 2
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = 0.01
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [0.1]
;cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(5,30)/100.0])
cnn_weight_sparsity = [0.5]
cnn_kernel_size = [5]
cnn_out_channels = [5]
;cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(4,8)])
linear_n = 50
linear_percent_on = 0.1
weight_sparsity = 0.3

checkpoint_at_end = True
scheduler = None

# Include some different learning parameters to trigger early stopping. Run with n=4 to get early stopping results
[quickSearch]
iterations = 20
repetitions = 2
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = tune.grid_search([0.01, 0.05, 0.075, 0.1])
momentum = tune.grid_search([0.0, 0.25, 0.5, 0.75, 0.9])
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.sample_from(lambda spec: [np.random.randint(5,30)/100.0])
cnn_weight_sparsity = [0.5]
cnn_kernel_size = [5]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(4,8)])
linear_n = 50
linear_percent_on = 0.1
weight_sparsity = 0.3

checkpoint_at_end = True

[quickDense]
iterations = 3
batch_size = 4
batches_in_epoch = 20
first_epoch_batch_size = 4
batches_in_first_epoch = 2
test_batch_size = 4
test_batches_in_epoch = 2
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 3]
cnn_out_channels = [8, 8]
linear_n = 100
linear_percent_on = 1.0
weight_sparsity = 1.0

checkpoint_at_end = True
scheduler = None

# One layer sparse network
[tinySparse1]
iterations = 15
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.01, 0.05, 0.1])
momentum = tune.grid_search([0.0, 0.5, 0.9])
learning_rate_gamma = tune.grid_search([0.7, 0.8, 0.9, 0.95, 1.0])

network_type = tiny_sparse
cnn_percent_on = [0.2]
cnn_out_channels = [8]
linear_n = 100
linear_percent_on = 0.2
weight_sparsity = 0.4

stop = {"stop": 1}


# One layer sparse network
# Best were cnn_out_channels=64, n=200.
[tinySparse11]
iterations = 20
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([[0.3], [0.2], [0.1]])
cnn_out_channels = tune.grid_search([[8], [16], [32], [64]])
linear_n = tune.grid_search([100, 200, 400])
linear_percent_on = tune.grid_search([0.1, 0.2])
weight_sparsity = 0.4

stop = {"stop": 1}

[tinySparse1Decent]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [0.3]
cnn_weight_sparsity = [0.5]
cnn_kernel_size = [5]
cnn_out_channels = [64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
;model_filename = "tinySparse1Decent.pth"


# Two layer sparse network
[tinySparse2Search]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 2000
test_batch_size = 64
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([[0.2, 0.3], [0.2, 0.1], [0.1, 0.2]])
cnn_out_channels = tune.grid_search([[64, 32], [64, 64], [32, 32], [32, 64]])
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}


# Two layer sparse network
[tinySparse2]
iterations = 50
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 2000
test_batch_size = 64
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [0.2, 0.2]
cnn_out_channels = [64, 64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True
model_filename = "tinySparse2.pt"

[tinyDense]
iterations = 10
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [1.0, 1.0]
cnn_out_channels = [8, 8]
linear_n = 100
linear_percent_on = 1.0
weight_sparsity = 1.0


# A one layer dense network
[dense1]
iterations = 10
batch_size = 32
batches_in_epoch = 500
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.01
momentum = 0.9
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [1.0]
cnn_out_channels = [8]
linear_n = 100
linear_percent_on = 1.0
weight_sparsity = 1.0

# Investigate various options for the first layer
[tinyOptionsL1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([ [0.2], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.5], [1.0] ])
cnn_kernel_size = tune.grid_search([ [3], [5] ])
cnn_out_channels = [64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True


# Investigate more options for the first layer
[tinyOptions2L1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([ [0.05], [0.1], [0.2], [0.3], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.3], [0.5], [1.0] ])
cnn_kernel_size = [5]
cnn_out_channels = [64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate more options for the first layer
[tinyOptions3L1]
iterations = 20
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([ [0.05], [0.1], [0.2], [0.3], [1.0] ])
cnn_weight_sparsity = tune.grid_search([ [0.3], [0.5], [1.0] ])
cnn_kernel_size = [5]
cnn_out_channels = [96]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for the second layer
[tinyOptions1L2]
iterations = 40
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = tune.grid_search([ [0.2, 1.0], [0.2, 0.2], [0.2, 0.3],])
cnn_weight_sparsity = tune.grid_search([ [0.5, 0.5], [0.5, 1.0] ])
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for a dense two layer
[bestDense2Layer]
iterations = 40
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.05
momentum = 0.5
learning_rate_gamma = 0.9

network_type = tiny_sparse
cnn_percent_on = [1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_percent_on = 1.0
weight_sparsity = 1.0

stop = {"stop": 1}
checkpoint_at_end = True

# Investigate options for a dense two layer
[dense2LayerOptions]
iterations = 60
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.05, 0.025, 0.01])
momentum = tune.grid_search([0.75, 0.9])
learning_rate_gamma = tune.grid_search([0.9, 0.95])

network_type = tiny_sparse
cnn_percent_on = [1.0, 1.0]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_percent_on = 1.0
weight_sparsity = 1.0

stop = {"stop": 1}
checkpoint_at_end = True

# Search learning rates
[tinyOptions2L2]
iterations = 60
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = tune.grid_search([0.05, 0.1, 0.025])
momentum = tune.grid_search([0.0, 0.25, 0.5, 0.75])
learning_rate_gamma = tune.grid_search([0.85, 0.9, 0.95])

network_type = tiny_sparse
cnn_percent_on = [0.2, 0.2]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True
scheduler = None

[bestSparse2Layer]
iterations = 200
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.1
momentum = 0.5
learning_rate_gamma = 0.95

network_type = tiny_sparse
cnn_percent_on = [0.2, 0.2]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = 800
linear_percent_on = 0.2
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True
scheduler = None

# Search linear layer
[tinyOptions3L2]
iterations = 80
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 32
test_batches_in_epoch = 500
learning_rate = 0.1
momentum = 0.5
learning_rate_gamma = 0.95

network_type = tiny_sparse
cnn_percent_on = [0.2, 0.2]
cnn_weight_sparsity = [1.0, 1.0]
cnn_kernel_size = [5, 5]
cnn_out_channels = [64, 64]
linear_n = tune.grid_search([200, 400, 600, 800])
linear_percent_on = tune.grid_search([0.1, 0.2, 0.05, 0.25])
weight_sparsity = 0.3

stop = {"stop": 1}
checkpoint_at_end = True


[sparse3Layer]
iterations = 200
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 128
test_batches_in_epoch = 500
learning_rate = 0.1
momentum = 0.5
learning_rate_gamma = 0.97

network_type = tiny_sparse
cnn_percent_on = [0.2, 0.2, 0.2]
cnn_weight_sparsity = [1.0, 1.0, 1.0]
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = [64, 64, 64]
linear_n = 200
linear_percent_on = 0.1
weight_sparsity = 1.0

stop = {"stop": 1}
checkpoint_at_end = True
scheduler = None


[sparse3LayerOptions]
iterations = 200
repetitions = 10
seed = 42
batch_size = 64
batches_in_epoch = 250
first_epoch_batch_size = 4
batches_in_first_epoch = 600
test_batch_size = 128
test_batches_in_epoch = 500
learning_rate = 0.1
momentum = 0.5
learning_rate_gamma = 0.97

network_type = tiny_sparse
cnn_percent_on = tune.sample_from(lambda spec: [np.random.uniform(0.05,0.3), np.random.uniform(0.05,0.3), np.random.uniform(0.05,0.3)])
cnn_weight_sparsity = tune.sample_from(lambda spec: [np.random.uniform(0.2,1.0), np.random.uniform(0.2,1.0), np.random.uniform(0.2,1.0)])
cnn_kernel_size = [5, 5, 3]
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(32,128), np.random.randint(32,128), np.random.randint(32,128)])
linear_n = tune.sample_from(lambda spec: np.random.randint(200,800))
linear_percent_on = tune.sample_from(lambda spec: np.random.uniform(0.1,0.3))
weight_sparsity = tune.sample_from(lambda spec: np.random.uniform(0.1,1.0))

stop = {"stop": 1}
checkpoint_at_end = True

