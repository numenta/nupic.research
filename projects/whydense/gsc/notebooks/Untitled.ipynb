{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio_transforms import (\n",
    "    AddNoise,\n",
    "    ChangeAmplitude,\n",
    "    ChangeSpeedAndPitchAudio,\n",
    "    DeleteSTFT,\n",
    "    FixAudioLength,\n",
    "    FixSTFTDimension,\n",
    "    LoadAudio,\n",
    "    StretchAudioOnSTFT,\n",
    "    TimeshiftAudioOnSTFT,\n",
    "    ToMelSpectrogram,\n",
    "    ToMelSpectrogramFromSTFT,\n",
    "    ToSTFT,\n",
    "    ToTensor,\n",
    "    Unsqueeze,\n",
    ")\n",
    "from nupic.torch.models.sparse_cnn import gsc_sparse_cnn, gsc_super_sparse_cnn\n",
    "from nupic.torch.modules import rezero_weights, update_boost_strength\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.research.frameworks.pytorch.model_utils import (\n",
    "    count_nonzero_params,\n",
    "    evaluate_model,\n",
    "    set_random_seed,\n",
    "    train_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/afisher/nta/nupic.torch/examples/gsc/\"\n",
    "config_path = \"/Users/afisher/nta/nupic.research/projects/whydense/gsc/\"\n",
    "sys.path.append(data_path)\n",
    "sys.path.append(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_GAMMA = 0.9\n",
    "MOMENTUM = 0.0\n",
    "EPOCHS = 30\n",
    "FIRST_EPOCH_BATCH_SIZE = 4\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 1000\n",
    "TEST_BATCH_SIZE = 1000\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "LABELS = tuple([\"unknown\", \"silence\", \"zero\", \"one\", \"two\", \"three\", \"four\",\n",
    "                \"five\", \"six\", \"seven\", \"eight\", \"nine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = Path(\"data\")\n",
    "EXTRACTPATH = DATAPATH / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, loader, optimizer, criterion, device):\n",
    "#     \"\"\"\n",
    "#     Train the model using given dataset loader.\n",
    "#     Called on every epoch.\n",
    "#     :param model: pytorch model to be trained\n",
    "#     :type model: torch.nn.Module\n",
    "#     :param loader: DataLoader configured for the epoch.\n",
    "#     :type loader: :class:`torch.utils.data.DataLoader`\n",
    "#     :param optimizer: Optimizer object used to train the model.\n",
    "#     :type optimizer: :class:`torch.optim.Optimizer`\n",
    "#     :param criterion: loss function to use\n",
    "#     :type criterion: function\n",
    "#     :param device:\n",
    "#     :type device: :class:`torch.device`\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     for data, target in tqdm(loader, desc=\"Train\", leave=False):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nupic.research.frameworks.pytorch.dataset_utils import PreprocessedDataset\n",
    "from nupic.research.frameworks.pytorch.model_utils import (\n",
    "    count_nonzero_params,\n",
    "    evaluate_model,\n",
    "    set_random_seed,\n",
    "    train_model,\n",
    ")\n",
    "from nupic.research.support import parse_config\n",
    "\n",
    "from nupic.research.frameworks.pytorch.models.le_sparse_net import LeSparseNet\n",
    "from nupic.research.frameworks.pytorch.models.resnet_models import resnet9\n",
    "from nupic.torch.models.sparse_cnn import GSCSparseCNN, GSCSuperSparseCNN\n",
    "from nupic.torch.modules import rezero_weights, update_boost_strength\n",
    "\n",
    "\n",
    "def get_logger(name, verbose):\n",
    "    \"\"\"Configure Logger based on verbose level (0: ERROR, 1: INFO, 2: DEBUG)\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    if verbose == 0:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    elif verbose == 1:\n",
    "        logger.setLevel(logging.INFO)\n",
    "    else:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "class SparseSpeechExperiment(object):\n",
    "    \"\"\"This experiment tests the Google Speech Commands dataset, available\n",
    "    here:\n",
    "    http://download.tensorflow.org/data/speech_commands_v0.01.tar\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Called once at the beginning of each experiment.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.logger = get_logger(config[\"name\"], config.get(\"verbose\", 2))\n",
    "        self.logger.debug(\"Config: %s\", config)\n",
    "\n",
    "        # Setup random seed\n",
    "        seed = config[\"seed\"]\n",
    "        set_random_seed(seed)\n",
    "\n",
    "        # Get our directories correct\n",
    "        self.data_dir = config[\"data_dir\"]\n",
    "\n",
    "        # Configure Model\n",
    "        self.model_type = config[\"model_type\"]\n",
    "        self.num_classes = 12\n",
    "        self.log_interval = config[\"log_interval\"]\n",
    "        self.batches_in_epoch = config[\"batches_in_epoch\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.background_noise_dir = config[\"background_noise_dir\"]\n",
    "        self.noise_values = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "        self.load_datasets()\n",
    "\n",
    "        if self.model_type == \"le_sparse\":\n",
    "            model = LeSparseNet(\n",
    "                input_shape=config.get(\"input_shape\", (1, 32, 32)),\n",
    "                cnn_out_channels=config[\"cnn_out_channels\"],\n",
    "                cnn_activity_percent_on=config[\"cnn_percent_on\"],\n",
    "                cnn_weight_percent_on=config[\"cnn_weight_sparsity\"],\n",
    "                linear_n=config[\"linear_n\"],\n",
    "                linear_activity_percent_on=config[\"linear_percent_on\"],\n",
    "                linear_weight_percent_on=config[\"weight_sparsity\"],\n",
    "                boost_strength=config[\"boost_strength\"],\n",
    "                boost_strength_factor=config[\"boost_strength_factor\"],\n",
    "                use_batch_norm=config[\"use_batch_norm\"],\n",
    "                dropout=config.get(\"dropout\", 0.0),\n",
    "                num_classes=self.num_classes,\n",
    "                k_inference_factor=config[\"k_inference_factor\"],\n",
    "                activation_fct_before_max_pool=config.get(\n",
    "                    \"activation_fct_before_max_pool\", False),\n",
    "                consolidated_sparse_weights=config.get(\n",
    "                    \"consolidated_sparse_weights\", False),\n",
    "                use_kwinners_local=config.get(\"use_kwinner_local\", False),\n",
    "            )\n",
    "\n",
    "        elif self.model_type == \"resnet9\":\n",
    "            model = resnet9(\n",
    "                num_classes=self.num_classes, in_channels=1\n",
    "            )\n",
    "\n",
    "        elif self.model_type == \"gsc_sparse_cnn\":\n",
    "            model = GSCSparseCNN()\n",
    "\n",
    "        elif self.model_type == \"gsc_super_sparse_cnn\":\n",
    "            model = GSCSuperSparseCNN()\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown model type: \" + self.model_type)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.logger.debug(\"use_cuda %s\", self.use_cuda)\n",
    "        if self.use_cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            model = model.cuda()\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.logger.debug(\"device %s\", self.device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.logger.debug(\"Using %s GPUs\", torch.cuda.device_count())\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        self.model = model\n",
    "        self.logger.debug(\"Model: %s\", self.model)\n",
    "        self.logger.debug(\"Model non-zero params: %s\", count_nonzero_params(self.model))\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.optimizer = self.create_optimizer(config, self.model)\n",
    "        self.lr_scheduler = self.create_learning_rate_scheduler(config, self.optimizer)\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        checkpoint_path = os.path.join(checkpoint_path, \"model.pt\")\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        return checkpoint_path\n",
    "\n",
    "    def restore(self, checkpoint_path):\n",
    "        checkpoint_path = os.path.join(checkpoint_path, \"model.pt\")\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(checkpoint_path, map_location=self.device)\n",
    "        )\n",
    "\n",
    "    def create_learning_rate_scheduler(self, params, optimizer):\n",
    "        \"\"\"Creates the learning rate scheduler and attach the optimizer.\"\"\"\n",
    "        lr_scheduler = params.get(\"lr_scheduler\", None)\n",
    "        if lr_scheduler is None:\n",
    "            return None\n",
    "\n",
    "        if lr_scheduler == \"StepLR\":\n",
    "            lr_scheduler_params = (\n",
    "                \"{'step_size': 1, 'gamma':\" + str(params[\"learning_rate_factor\"]) + \"}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            lr_scheduler_params = params.get(\"lr_scheduler_params\", None)\n",
    "            if lr_scheduler_params is None:\n",
    "                raise ValueError(\n",
    "                    \"Missing 'lr_scheduler_params' for {}\".format(lr_scheduler)\n",
    "                )\n",
    "\n",
    "        # Get lr_scheduler class by name\n",
    "        clazz = eval(\"torch.optim.lr_scheduler.{}\".format(lr_scheduler))\n",
    "\n",
    "        # Parse scheduler parameters from config\n",
    "        lr_scheduler_params = eval(lr_scheduler_params)\n",
    "\n",
    "        return clazz(optimizer, **lr_scheduler_params)\n",
    "\n",
    "    def create_optimizer(self, params, model):\n",
    "        \"\"\"Create a new instance of the optimizer.\"\"\"\n",
    "        lr = params[\"learning_rate\"]\n",
    "        print(\"Creating optimizer with learning rate=\", lr)\n",
    "        if params[\"optimizer\"] == \"SGD\":\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=params[\"momentum\"],\n",
    "                weight_decay=params[\"weight_decay\"],\n",
    "            )\n",
    "        elif params[\"optimizer\"] == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise LookupError(\"Incorrect optimizer value\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train(self, epoch):\n",
    "        \"\"\"Train one epoch of this model by iterating through mini batches.\n",
    "        An epoch ends after one pass through the training set, or if the\n",
    "        number of mini batches exceeds the parameter \"batches_in_epoch\".\n",
    "        \"\"\"\n",
    "        self.logger.info(\"epoch: %s\", epoch)\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        self.logger.info(\n",
    "            \"Learning rate: %s\",\n",
    "            self.learning_rate\n",
    "            if self.lr_scheduler is None\n",
    "            else self.lr_scheduler.get_lr(),\n",
    "        )\n",
    "\n",
    "        self.pre_epoch()\n",
    "        train_model(self.model, self.train_loader, self.optimizer, self.device,\n",
    "                    batches_in_epoch=self.batches_in_epoch)\n",
    "        self.post_epoch()\n",
    "\n",
    "        self.logger.info(\"training duration: %s\", time.time() - t0)\n",
    "\n",
    "    def post_epoch(self):\n",
    "        self.model.apply(rezero_weights)\n",
    "        self.lr_scheduler.step()\n",
    "        self.train_loader.dataset.load_next()\n",
    "\n",
    "    def pre_epoch(self):\n",
    "        self.model.apply(update_boost_strength)\n",
    "\n",
    "    def test(self, test_loader=None):\n",
    "        \"\"\"Test the model using the given loader and return test metrics.\"\"\"\n",
    "        if test_loader is None:\n",
    "            test_loader = self.test_loader\n",
    "\n",
    "        ret = evaluate_model(self.model, test_loader, self.device)\n",
    "        ret[\"mean_accuracy\"] = 100.0 * ret[\"mean_accuracy\"]\n",
    "\n",
    "        entropy = self.entropy()\n",
    "        ret.update({\n",
    "            \"entropy\": float(entropy),\n",
    "            \"total_samples\": len(test_loader.sampler),\n",
    "            \"non_zero_parameters\": count_nonzero_params(self.model)[1],\n",
    "        })\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def entropy(self):\n",
    "        \"\"\"Returns the current entropy.\"\"\"\n",
    "        entropy = 0\n",
    "        for module in self.model.modules():\n",
    "            if module == self.model:\n",
    "                continue\n",
    "            if hasattr(module, \"entropy\"):\n",
    "                entropy += module.entropy()\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Run validation.\"\"\"\n",
    "        if self.validation_loader:\n",
    "            return self.test(self.validation_loader)\n",
    "        return None\n",
    "\n",
    "    def run_noise_tests(self):\n",
    "        \"\"\"\n",
    "        Test the model with different noise values and return test metrics.\n",
    "        Loads pre-generated noise dataset with noise transforms included\n",
    "        \"\"\"\n",
    "        ret = {}\n",
    "        for noise in self.noise_values:\n",
    "            noise_qualifier = \"{:02d}\".format(int(100 * noise))\n",
    "            self.test_loader.dataset.load_qualifier(noise_qualifier)\n",
    "            ret[noise] = self.test(self.test_loader)\n",
    "        return ret\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        GSC specifies specific files to be used as training, test, and validation.\n",
    "        We assume the data has already been processed using the pre-processing scripts\n",
    "        here: https://github.com/numenta/nupic.torch/tree/master/examples/gsc\n",
    "        \"\"\"\n",
    "        validation_dataset = PreprocessedDataset(\n",
    "            cachefilepath=self.data_dir,\n",
    "            basename=\"gsc_valid\",\n",
    "            qualifiers=[\"\"],\n",
    "        )\n",
    "\n",
    "        test_dataset = PreprocessedDataset(\n",
    "            cachefilepath=self.data_dir,\n",
    "            basename=\"gsc_test_noise\",\n",
    "            qualifiers=[\"{:02d}\".format(int(100 * n)) for n in self.noise_values],\n",
    "        )\n",
    "        train_dataset = PreprocessedDataset(\n",
    "            cachefilepath=self.data_dir,\n",
    "            basename=\"gsc_train\",\n",
    "            qualifiers=range(30),\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        self.validation_loader = DataLoader(\n",
    "            validation_dataset, batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../experiments.cfg\"\n",
    "configs = parse_config(open(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"sparseCNN2\"\n",
    "config_exp = configs[exp]\n",
    "config_exp[\"name\"] = exp\n",
    "config_exp[\"seed\"] = 42\n",
    "config_exp[\"data_dir\"] = \"/Users/afisher/nta/nupic.torch/examples/gsc/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer with learning rate= 0.01\n"
     ]
    }
   ],
   "source": [
    "experiment = SparseSpeechExperiment(config=config_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-45f06a8f5581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'epoch'"
     ]
    }
   ],
   "source": [
    "experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_correct': 2389,\n",
       " 'mean_loss': 0.22239979163256,\n",
       " 'mean_accuracy': 93.6128526645768,\n",
       " 'entropy': 26589.712890625,\n",
       " 'total_samples': 2552,\n",
       " 'non_zero_parameters': 194452}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 16s, sys: 27.5 s, total: 13min 43s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time experiment.train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26589.7129)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelclass = gsc_super_sparse_cnn\n",
    "model = modelclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.research.frameworks.pytorch.dataset_utils import PreprocessedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PreprocessedDataset(data_path+'data/',\n",
    "                                \"gsc_train\",\n",
    "                                range(30))\n",
    "\n",
    "noise_values = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "test_dataset = PreprocessedDataset(\n",
    "    cachefilepath=data_path+'data/',\n",
    "    basename=\"gsc_test_noise\",\n",
    "    qualifiers=[\"{:02d}\".format(int(100 * n)) for n in noise_values],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(sgd, step_size=1, gamma=LEARNING_RATE_GAMMA)\n",
    "criterion = F.nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, sgd, criterion, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader=None):\n",
    "    \"\"\"Test the model using the given loader and return test metrics.\"\"\"\n",
    "    if test_loader is None:\n",
    "        test_loader = test_loader\n",
    "\n",
    "    ret = evaluate_model(model, test_loader, device=\"cpu\")\n",
    "    ret[\"mean_accuracy\"] = 100.0 * ret[\"mean_accuracy\"]\n",
    "\n",
    "    ent_ = entropy()\n",
    "    ret.update({\n",
    "        \"entropy\": float(ent_),\n",
    "        \"total_samples\": len(test_loader.sampler),\n",
    "        \"non_zero_parameters\": count_nonzero_params(model)[1],\n",
    "    })\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def entropy():\n",
    "    \"\"\"Returns the current entropy.\"\"\"\n",
    "    entropy = 0\n",
    "    for module in model.modules():\n",
    "        if module == model:\n",
    "            continue\n",
    "        if hasattr(module, \"entropy\"):\n",
    "            entropy += module.entropy()\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_correct': 2147,\n",
       " 'mean_loss': 0.5673229236587836,\n",
       " 'mean_accuracy': 84.13009404388715,\n",
       " 'entropy': 26602.62890625,\n",
       " 'total_samples': 2552,\n",
       " 'non_zero_parameters': 411877}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'denseCNN2': {'path': '~/nta/results/gsc',\n",
       "  'data_dir': '~/nta/data/gsc_preprocessed',\n",
       "  'verbose': 2,\n",
       "  'num_classes': 12,\n",
       "  'checkpoint_freq': 1,\n",
       "  'checkpoint_at_end': False,\n",
       "  'gpu_percentage': 1.0,\n",
       "  'repetitions': 20,\n",
       "  'seed': 'tune.sample_from(lambda spec: np.random.randint(1000, 100000))',\n",
       "  'iterations': 30,\n",
       "  'batch_size': 64,\n",
       "  'batches_in_epoch': 100000,\n",
       "  'test_batch_size': 1000,\n",
       "  'learning_rate': 0.01,\n",
       "  'weight_decay': 0.01,\n",
       "  'learning_rate_factor': 0.8,\n",
       "  'use_batch_norm': True,\n",
       "  'momentum': 0.9,\n",
       "  'dropout': 'tune.grid_search([0.0, 0.1, 0.2])',\n",
       "  'boost_strength': 1.5,\n",
       "  'boost_strength_factor': 0.9,\n",
       "  'linear_n': (1000,),\n",
       "  'linear_percent_on': (1.0,),\n",
       "  'k_inference_factor': 1.0,\n",
       "  'log_interval': 400,\n",
       "  'test_noise_every_epoch': False,\n",
       "  'background_noise_dir': '_background_noise_',\n",
       "  'optimizer': 'SGD',\n",
       "  'lr_scheduler': 'StepLR',\n",
       "  'model_type': 'le_sparse',\n",
       "  'activation_fct_before_max_pool': True,\n",
       "  'input_shape': (1, 32, 32),\n",
       "  'cnn_out_channels': (64, 64),\n",
       "  'cnn_percent_on': (1.0, 1.0),\n",
       "  'cnn_weight_sparsity': (1.0, 1.0),\n",
       "  'weight_sparsity': (1.0,)},\n",
       " 'sparseCNN2': {'path': '~/nta/results/gsc',\n",
       "  'data_dir': '~/nta/data/gsc_preprocessed',\n",
       "  'verbose': 2,\n",
       "  'num_classes': 12,\n",
       "  'checkpoint_freq': 1,\n",
       "  'checkpoint_at_end': False,\n",
       "  'gpu_percentage': 1.0,\n",
       "  'repetitions': 20,\n",
       "  'seed': 'tune.sample_from(lambda spec: np.random.randint(1000, 100000))',\n",
       "  'iterations': 30,\n",
       "  'batch_size': 16,\n",
       "  'batches_in_epoch': 100000,\n",
       "  'test_batch_size': 1000,\n",
       "  'learning_rate': 0.01,\n",
       "  'weight_decay': 0.01,\n",
       "  'learning_rate_factor': 0.9,\n",
       "  'use_batch_norm': True,\n",
       "  'momentum': 0.0,\n",
       "  'dropout': 0.0,\n",
       "  'boost_strength': 1.5,\n",
       "  'boost_strength_factor': 0.9,\n",
       "  'linear_n': (1000,),\n",
       "  'linear_percent_on': (0.1,),\n",
       "  'k_inference_factor': 1.0,\n",
       "  'log_interval': 400,\n",
       "  'test_noise_every_epoch': False,\n",
       "  'background_noise_dir': '_background_noise_',\n",
       "  'optimizer': 'SGD',\n",
       "  'lr_scheduler': 'StepLR',\n",
       "  'model_type': 'le_sparse',\n",
       "  'activation_fct_before_max_pool': True,\n",
       "  'input_shape': (1, 32, 32),\n",
       "  'cnn_out_channels': (64, 64),\n",
       "  'cnn_percent_on': (0.095, 0.125),\n",
       "  'cnn_weight_sparsity': (0.5, 0.2),\n",
       "  'weight_sparsity': (0.1,)},\n",
       " 'denseSmall1': {'path': '~/nta/results/gsc',\n",
       "  'data_dir': '~/nta/data/gsc_preprocessed',\n",
       "  'verbose': 2,\n",
       "  'num_classes': 12,\n",
       "  'checkpoint_freq': 1,\n",
       "  'checkpoint_at_end': False,\n",
       "  'gpu_percentage': 1.0,\n",
       "  'repetitions': 20,\n",
       "  'seed': 'tune.sample_from(lambda spec: np.random.randint(1000, 100000))',\n",
       "  'iterations': 30,\n",
       "  'batch_size': 64,\n",
       "  'batches_in_epoch': 5121,\n",
       "  'test_batch_size': 1000,\n",
       "  'learning_rate': 0.01,\n",
       "  'weight_decay': 0.01,\n",
       "  'learning_rate_factor': 0.8,\n",
       "  'use_batch_norm': True,\n",
       "  'momentum': 0.9,\n",
       "  'dropout': 0.0,\n",
       "  'boost_strength': 1.5,\n",
       "  'boost_strength_factor': 0.9,\n",
       "  'linear_n': (300,),\n",
       "  'linear_percent_on': (1.0,),\n",
       "  'k_inference_factor': 1.0,\n",
       "  'log_interval': 100,\n",
       "  'test_noise_every_epoch': False,\n",
       "  'background_noise_dir': '_background_noise_',\n",
       "  'optimizer': 'SGD',\n",
       "  'lr_scheduler': 'StepLR',\n",
       "  'model_type': 'le_sparse',\n",
       "  'activation_fct_before_max_pool': True,\n",
       "  'input_shape': (1, 32, 32),\n",
       "  'cnn_out_channels': (32, 32),\n",
       "  'cnn_percent_on': (1.0, 1.0),\n",
       "  'cnn_weight_sparsity': (1.0, 1.0),\n",
       "  'weight_sparsity': (1.0,)},\n",
       " 'denseSmall2': {'path': '~/nta/results/gsc',\n",
       "  'data_dir': '~/nta/data/gsc_preprocessed',\n",
       "  'verbose': 2,\n",
       "  'num_classes': 12,\n",
       "  'checkpoint_freq': 1,\n",
       "  'checkpoint_at_end': False,\n",
       "  'gpu_percentage': 1.0,\n",
       "  'repetitions': 20,\n",
       "  'seed': 'tune.sample_from(lambda spec: np.random.randint(1000, 100000))',\n",
       "  'iterations': 30,\n",
       "  'batch_size': 64,\n",
       "  'batches_in_epoch': 5121,\n",
       "  'test_batch_size': 1000,\n",
       "  'learning_rate': 0.01,\n",
       "  'weight_decay': 0.01,\n",
       "  'learning_rate_factor': 0.8,\n",
       "  'use_batch_norm': True,\n",
       "  'momentum': 0.9,\n",
       "  'dropout': 0.0,\n",
       "  'boost_strength': 1.5,\n",
       "  'boost_strength_factor': 0.9,\n",
       "  'linear_n': (300,),\n",
       "  'linear_percent_on': (1.0,),\n",
       "  'k_inference_factor': 1.0,\n",
       "  'log_interval': 100,\n",
       "  'test_noise_every_epoch': False,\n",
       "  'background_noise_dir': '_background_noise_',\n",
       "  'optimizer': 'SGD',\n",
       "  'lr_scheduler': 'StepLR',\n",
       "  'model_type': 'le_sparse',\n",
       "  'activation_fct_before_max_pool': True,\n",
       "  'input_shape': (1, 32, 32),\n",
       "  'cnn_out_channels': (32, 64),\n",
       "  'cnn_percent_on': (1.0, 1.0),\n",
       "  'cnn_weight_sparsity': (1.0, 1.0),\n",
       "  'weight_sparsity': (1.0,)},\n",
       " 'SuperSparseCNN2': {'path': '~/nta/results/gsc',\n",
       "  'data_dir': '~/nta/data/gsc_preprocessed',\n",
       "  'verbose': 2,\n",
       "  'num_classes': 12,\n",
       "  'checkpoint_freq': 1,\n",
       "  'checkpoint_at_end': False,\n",
       "  'gpu_percentage': 1.0,\n",
       "  'repetitions': 20,\n",
       "  'seed': 'tune.sample_from(lambda spec: np.random.randint(1000, 100000))',\n",
       "  'iterations': 30,\n",
       "  'batch_size': 16,\n",
       "  'batches_in_epoch': 100000,\n",
       "  'test_batch_size': 1000,\n",
       "  'learning_rate': 0.01,\n",
       "  'weight_decay': 0.01,\n",
       "  'learning_rate_factor': 0.9,\n",
       "  'use_batch_norm': True,\n",
       "  'momentum': 0.0,\n",
       "  'dropout': 0.0,\n",
       "  'boost_strength': 1.5,\n",
       "  'boost_strength_factor': 0.9,\n",
       "  'linear_n': (1500,),\n",
       "  'linear_percent_on': (0.1,),\n",
       "  'k_inference_factor': 1.0,\n",
       "  'log_interval': 400,\n",
       "  'test_noise_every_epoch': False,\n",
       "  'background_noise_dir': '_background_noise_',\n",
       "  'optimizer': 'SGD',\n",
       "  'lr_scheduler': 'StepLR',\n",
       "  'model_type': 'le_sparse',\n",
       "  'activation_fct_before_max_pool': True,\n",
       "  'input_shape': (1, 32, 32),\n",
       "  'cnn_out_channels': (64, 64),\n",
       "  'cnn_percent_on': (0.095, 0.125),\n",
       "  'cnn_weight_sparsity': (0.5, 0.2),\n",
       "  'weight_sparsity': (0.05,)}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_config(open('../experiments.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgparser = configparser.ConfigParser()\n",
    "cfgparser.read_file(open(\"../experiments.cfg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: ansi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d8e7746d0989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiledata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../experiments.cfg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ansi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: ansi"
     ]
    }
   ],
   "source": [
    "filedata = open(\"../experiments.cfg\", encoding='ansi').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afisher/miniconda3/envs/nupic.research/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: This method will be removed in future versions.  Use 'parser.read_file()' instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cfgparser.readfp(codecs.open('../experiments.cfg', 'r', 'utf-8-sig'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgparser.read_file(open('../experiments.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denseCNN2', 'sparseCNN2', 'denseSmall1', 'denseSmall2', 'SuperSparseCNN2']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfgparser.sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.tune"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
