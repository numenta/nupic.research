# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------
[DEFAULT]
; Uncomment to save results to S3
;upload_dir = "s3://bucketname/ray/whydense/gsc"
;sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

path = ~/nta/gsc_results
data_dir = ~/nta/data/gsc_preprocessed
verbose = 2
checkpoint_freq = 1
checkpoint_at_end = False
gpu_percentage = 1.0

; Uncomment to average over multiple seeds
repetitions = 1
seed = 42
;repetitions = 10
;seed = tune.sample_from(lambda spec: np.random.randint(1, 10000))

iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
linear_n = 2000
linear_percent_on = 1.0
weight_sparsity = 0.50
cnn_weight_sparsity = 1.0
k_inference_factor = 1.0

log_interval = 1000         # how many minibatches to wait before logging
test_noise_every_epoch = False # If False, will only test noise at end

background_noise_dir = _background_noise_

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = StepLR

model_type = cnn  # cnn, resnet9, gsc_sparse_cnn, gsc_super_sparse_cnn
cnn_out_channels = (10,)
cnn_percent_on = 1.0
dropout = 0.5

run_noise_tests = False
run_noise_tests_best_model = True
count_nonzeros = False
save_every_epoch = False

input_shape = (1, 32, 32)

[quick]
linear_n = (100,)
linear_percent_on = (0.1,)
cnn_out_channels = (8, 8)
cnn_percent_on = (0.095, 0.125)
k_inference_factor = 1.5
iterations = 5
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.4
dropout = 0.0
log_interval = 400
batches_in_epoch = 200
batch_size = 16
model_type = cnn

# This gets between 96.5% and 97.5% accuracy
[sparseCNN2]
linear_n = (1000,)
linear_percent_on = (0.1,)
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.4
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = cnn

# This leads to about 92.6 % accuracy
[sparseNoBatchNorm]
linear_n = (1000,)
linear_percent_on = (0.1,)
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.95
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.4
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = cnn
use_batch_norm = False

[denseCNN2]
linear_n = (1000,)
linear_percent_on = (1.0,)
cnn_out_channels = (64, 64)
cnn_percent_on = (1.0, 1.0)
k_inference_factor = 1.5
iterations = 20
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = 1.0
dropout = tune.grid_search([0.0, 0.5])
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
model_type = cnn
use_batch_norm = True

; Larger sparse with lower weight sparsity
[SuperSparseCNN2]
linear_n = (1500,)
linear_percent_on = (0.067,)
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
k_inference_factor = 1.5
iterations = 25
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.1
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = cnn

