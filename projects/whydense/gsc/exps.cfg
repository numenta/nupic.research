# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------
[DEFAULT]
; Uncomment to save results to S3
;upload_dir = "s3://bucketname/ray/whydense/gsc"
;sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

path = ~/nta/results/gsc
data_dir = ~/nta/data/gsc_preprocessed
verbose = 2
num_classes = 12
checkpoint_freq = 1
checkpoint_at_end = False
gpu_percentage = 1.0

; Uncomment to average over multiple seeds
;repetitions = 1
;seed = 42
repetitions = 10
seed = tune.sample_from(lambda spec: np.random.randint(1, 10000))

iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
linear_n = 2000
linear_percent_on = 1.0
weight_sparsity = (0.50, )
cnn_weight_sparsity = (1.0, )
k_inference_factor = 1.0

log_interval = 1000         # how many minibatches to wait before logging
test_noise_every_epoch = False # If False, will only test noise at end

background_noise_dir = _background_noise_

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = StepLR

; cnn, resnet9, gsc_sparse_cnn, gsc_super_sparse_cnn
model_type = le_sparse
activation_fct_before_max_pool = True
cnn_out_channels = (10,)
cnn_percent_on = 1.0
dropout = 0.5

input_shape = (1, 32, 32)

[quick]
linear_n = (100,)
linear_percent_on = (0.1,)
cnn_out_channels = (8, 8)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 1.0)
k_inference_factor = 1.5
min_epoch_for_checkpoint = 3
iterations = 20
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = (0.4,)
dropout = 0.0
log_interval = 400
batches_in_epoch = 3
batch_size = 16
model_type = le_sparse

# This gets between 96.5% and 97.5% accuracy
# Deleted
[sparseCNN2]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.5% accuracy
[sparseCNN2WtSparsity]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse


# This gets around 96.5% accuracy
# Delete
[sparseCNN2WtSparsity2]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse


# This gets around 96.3% accuracy
[sparseCNN2WtSparsity3]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.1)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse


# This gets around 96.4% accuracy
[sparseCNN2WtSparsity4]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.05)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.6% accuracy
[sparseCNN2WtSparsity5]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.4, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.4,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.7 accuracy
[sparseCNN2WtSparsity6]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.4, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.3,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.3 accuracy
[sparseCNN2WtSparsity7]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.3, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.3,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 97.03 accuracy
[sparseCNN2WtSparsity8]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.4)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.7% accuracy
[sparseCNN2WtSparsity9]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse

# This gets around 96.9% accuracy and the best noise robustness so far.
[sparseCNN2WtSparsity10]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


# Reducing k inf factor to 1.0. This gets around 96.9% accuracy and has excellent robustness.
# New SuperSparseCNN2 candidate
[sparseCNN2WtSparsity11]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


# Moving activation function after max pool.
# This gets around 97% accuracy and has ok robustness.
[sparseCNN2WtSparsity12]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = False


# This has very sparse CNN layers but gets around 95.7 - 96% accuracy
# Delete.
[sparseCNN2WtSparsity13]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.05, 0.06)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = False

# Experiment with consolidated sparse weights, a variation of sparseCNN2WtSparsity12
# This gets around 96.9% accuracy and has ok robustness.
[sparseCNN2WtSparsity14]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = False
consolidated_sparse_weights = True

# Like sparseCNN2WtSparsity11 but with local k winners.
# Reducing k inf factor to 1.0.
# This gets around 97% accuracy and 12,323 robustness.
# delete
[sparseCNN2WtSparsity15]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
use_kwinner_local = True


# Like sparseCNN2WtSparsity15 but with local k winners and sparser k-winners
# k inf factor to 1.0.
# This gets around 96.9% accuracy and 1169 robustness.
# DELETE
[sparseCNN2WtSparsity16]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.1, 0.1)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
use_kwinner_local = True


# Like sparseCNN2WtSparsity15 but with local k winners and sparser k-winners
# k inf factor to 1.0.
# This gets around 96.9% accuracy and 1169 robustness.
# delete
[sparseCNN2WtSparsity17]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.1, 0.1)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
use_kwinner_local = True


# This one has 1000 linear units and 10% weight density.
# Reducing k inf factor to 1.0. This gets around 97% accuracy and
# has 12,471 robustness.  New sparseCNN2 candidate
[sparseCNN2WtSparsity18]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Larger linear layers
[sparseCNN2WtSparsity19]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.1, 0.1)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


# Large linear layers, sparser activations and weights
[sparseCNN2WtSparsity20]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.1, 0.1)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (2000,)
linear_percent_on = (0.05,)
weight_sparsity = (0.03,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Can a network with 1000 linear units but 5% weight density do ok?
# 97% accuracy but 11,930 robustness
[sparseCNN2WtSparsity21]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


# Experiment with consolidated sparse weights, a variation of
# sparseCNN2WtSparsity14 but with activation fct before max_pool
# Test Score   | Noise Score       | Noise Accuracy   | Total Entropy    | Nonzero Parameters
# 96.92 ± 0.13 | 11,099 ±  553.20  | 39.54 ±  1.97    | 26703.24 ± 18.17 | 161,080
[sparseCNN2WtSparsity23]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.6, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
consolidated_sparse_weights = True

# Experiment with even sparser consolidated sparse weights, a variation of
# sparseCNN2WtSparsity23
# Test Score   | Noise Score       | Noise Accuracy   | Total Entropy    | Nonzero Parameters
# 96.74 ± 0.24 | 11,524 ±  703.77  | 41.05 ±  2.51    | 26698.26 ± 20.99 | 150,840
[sparseCNN2WtSparsity24]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.6, 0.1)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
consolidated_sparse_weights = True


# Experiment with even sparser consolidated sparse weights, a variation of
# sparseCNN2WtSparsity24 with activation_fct_before_max_pool = False
# Test Score   | Noise Score       | Noise Accuracy   | Total Entropy    | Nonzero Parameters
[sparseCNN2WtSparsity25]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.6, 0.1)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = False
consolidated_sparse_weights = True

# Do we really need momentum? Does it help/hurt?
# Compare against sparseCNN2WtSparsity21
# Momentum seems to hurt:
;+------------------------+--------------+-----------------+------------------+----------------------+--------------+
;| Network                | Test Score   | Noise Score     | Noise Accuracy   | Nonzero Parameters   |   Num Trials |
;+========================+==============+=================+==================+======================+==============+
;| sparseCNN2WtSparsity21 | 97.02 ± 0.10 | 11,930 ± 829.67 | 42.50 ± 2.96     | 114,452              |           10 |
;+------------------------+--------------+-----------------+------------------+----------------------+--------------+
;| sparseCNN2MomentumTest | 95.60 ± 0.33 | 9,395 ± 1088.30 | 33.47 ± 3.88     | 114,452              |           10 |
;+------------------------+--------------+-----------------+------------------+----------------------+--------------+

[sparseCNN2MomentumTest]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.9
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16


# This one is like sparseCNN2WtSparsity18 but with a search for boost strength
#
[sparseCNN2WtBoostFactors2]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
iterations = 30
boost_strength = tune.grid_search([0.0, 1.0, 1.5, 2.0])
boost_strength_factor = tune.grid_search([0.9, 0.95, 0.97])
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


# This one uses the best boost factors from sparseCNN2WtBoostFactors2
# but with the best boost factors. Led to 12,300 noise accuracy
[sparseCNN2WtSparsity22]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.95
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True


; Larger sparse with lower weight sparsity
; Potentially the next supersparse
[SuperSparseCNN2]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05, )
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16


[denseCNN2]
linear_n = (1000,)
linear_percent_on = (1.0,)
cnn_out_channels = (64, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 20
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = tune.grid_search([0.0, 0.1, 0.2])
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
model_type = le_sparse
use_batch_norm = True

# Useful to see if any variation of size params helps. They don't but the next
# few experiments test that more thoroughly
# Deleted
[denseCNN2Size]
linear_n = tune.sample_from(lambda spec: [np.random.randint(200,1000), ])
linear_percent_on = (1.0,)
cnn_out_channels = tune.sample_from(lambda spec: [np.random.randint(8, 65), np.random.randint(8, 65)])
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True

[denseCNN2Small1]
linear_n = (300, )
linear_percent_on = (1.0,)
cnn_out_channels = (32, 32)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True

[denseCNN2Small2]
linear_n = (500, )
linear_percent_on = (1.0,)
cnn_out_channels = (32, 32)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True

# This became dense denseSmall2 in the paper
[denseCNN2Small3]
linear_n = (300, )
linear_percent_on = (1.0,)
cnn_out_channels = (32, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True

[denseCNN2Small4]
linear_n = (500, )
linear_percent_on = (1.0,)
cnn_out_channels = (64, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True


[denseCNN2Small5]
linear_n = (250, )
linear_percent_on = (1.0,)
cnn_out_channels = (16, 32)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = 0.0
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
use_batch_norm = True

# Code change moving ReLU to be after batch norm.
# This will be the new denseCNN2. Deleted.
[denseCNN2ReluAfterBN]
linear_n = (1000,)
linear_percent_on = (1.0,)
cnn_out_channels = (64, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = (1.0, )
dropout = tune.grid_search([0.0, 0.1, 0.2])
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
model_type = le_sparse
use_batch_norm = True


; Apply k-winners across the channels
[SuperSparseCNNLocal]
linear_n = (1500,)
linear_percent_on = (0.067,)
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 1.0)
k_inference_factor = 1.5
iterations = 25
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = (0.1, )
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
use_kwinner_local = True

# Experiment with consolidated sparse weights and k-winner local
# This gets around 96.55% accuracy
[BlockSparseCNNLocal]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.3)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.05,)
k_inference_factor = 1.0
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
consolidated_sparse_weights = True
use_kwinner_local = True
queue_trials = True
