{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "import torch\n",
    "from torchnlp.samplers import BPTTBatchSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from rsm_samplers import MNISTSequenceSampler, ptb_pred_sequence_collate\n",
    "from ptb_lstm import LSTMModel\n",
    "from lang_util import Corpus\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from importlib import reload \n",
    "from torch.utils.data import Sampler, BatchSampler\n",
    "import rsm\n",
    "from functools import reduce, partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('/Users/jgordon/nta/datasets/PTB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import rsm_samplers\n",
    "import rsm\n",
    "import lang_util\n",
    "reload(rsm_samplers)\n",
    "reload(rsm)\n",
    "reload(lang_util)\n",
    "\n",
    "BS = 10\n",
    "SEQL = 2\n",
    "VS = 10000\n",
    "EMB_DIM = 28\n",
    "\n",
    "bwe = lang_util.BitwiseWordEmbedding()\n",
    "\n",
    "vector_dict = {}\n",
    "for i in range(VS):\n",
    "    random_binary = torch.FloatTensor(EMB_DIM).uniform_() > 0.85\n",
    "    vector_dict[i] = random_binary\n",
    "\n",
    "model = rsm.RSMLayer(d_in=EMB_DIM, d_out=EMB_DIM, embed_dim=EMB_DIM)\n",
    "collate_fn = partial(rsm_samplers.ptb_pred_sequence_collate, vector_dict=bwe.embedding_dict)\n",
    "\n",
    "sampler = rsm_samplers.PTBSequenceSampler(corpus.train, batch_size=BS, seq_length=SEQL)\n",
    "loader = DataLoader(corpus.train,\n",
    "                       batch_sampler=sampler,\n",
    "                       collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 365,   27,   27,  108, 6220, 1015,  238,  108,  805, 3509],\n",
       "         [  27,   27, 2478,   32, 2577,  525,  220,  366,   32,  219]]),\n",
       " tensor([[  27,   27, 2478,   32, 2577,  525,  220,  366,   32,  219],\n",
       "         [ 453,   42,  229,  237,   42,   24,   26,   48, 3818,  152]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 28])\n",
      "torch.Size([1, 10, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# batch = next(iter(loader))\n",
    "# inputs, target, pred_target = batch\n",
    "# print(inputs.size())\n",
    "# hidden = model.init_hidden(BS)\n",
    "# x_a, hidden, x_b = model(inputs, hidden)\n",
    "# print(x_a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsm_samplers\n",
    "reload(rsm_samplers)\n",
    "\n",
    "from torch.utils.data import DataLoader, BatchSampler\n",
    "\n",
    "dataset = datasets.MNIST(\"~/nta/datasets\", download=True,\n",
    "                                               transform=transforms.Compose([\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                               ]),)\n",
    "\n",
    "sl = 12\n",
    "bs = 10\n",
    "sampler = rsm_samplers.MNISTSequenceSampler(dataset, batch_size=bs, sequences=[[0,1,2,3],[4,5,6,7],[9,8,7,6]], randomize_sequences=True)\n",
    "batch_sampler = BatchSampler(sampler, batch_size=sl * bs + 1, drop_last=True)\n",
    "\n",
    "collate_fn = partial(rsm_samplers.pred_sequence_collate, \n",
    "                     bsz=bs,\n",
    "                     seq_length=sl)\n",
    "loader = DataLoader(dataset,\n",
    "                               batch_sampler=batch_sampler,\n",
    "                               collate_fn=collate_fn)\n",
    "model = rsm.RSMLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1001001010010101101101011010'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BitwiseWordEmbedding(object):\n",
    "\n",
    "    def __init__(self, vocab_size=10000, dim=28):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dict = {}\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        for i in range(self.vocab_size):\n",
    "            self.embedding_dict[i] = self.embed(i)\n",
    "\n",
    "    def embed(self, i):\n",
    "        first = \"{0:b}\".format(i).zfill(self.dim // 2)\n",
    "        return first + self.inverse(first)\n",
    "\n",
    "    def inverse(self, binstr):\n",
    "        return ''.join('1' if x == '0' else '0' for x in binstr)\n",
    "\n",
    "bwe = BitwiseWordEmbedding()\n",
    "\n",
    "bwe.embed(9381)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
