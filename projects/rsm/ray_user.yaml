# An unique identifier for the head node and workers of this cluster
# Usually your user name
cluster_name: jgordon

# Autscaler config. 
# See https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml
min_workers: 0
max_workers: 1 
initial_workers: 0
target_utilization_fraction: 0.8
idle_timeout_minutes: 5

provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2c # us-west-2a,us-west-2b,us-west-2c

auth:
    ssh_user: ubuntu
    # Avaialble from "Numenta Engineering Secrets" google drive 
    ssh_private_key: ~/.ssh/ray-autoscaler_us-west-2.pem 

# For more documentation on available fields for both head and worker nodes, 
# See http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
head_node:
    KeyName: ray-autoscaler_us-west-2 

    # 4 CPU, 1 GPU (NVIDIA K80)
    InstanceType: p3.2xlarge 
    
    # 100G disk size
    BlockDeviceMappings:
    - DeviceName: /dev/sda1
      Ebs:
        VolumeSize: 100

    # DL AMI 22, Ray 0.6.6, pytorch 1.1.0, nupic.torch, nupic.research
    ImageId: ami-04b12564224b76fe2 

    # Using "basic_server" security group
    # It requires VPN when accessing this node from outside the office
    SecurityGroupIds:
        - sg-04ad5dbdb293d7ccb # ray-cluster 
    SubnetIds: 
        # - subnet-71025a14 # oregon-default (us-west-2b)
        - subnet-0f5e36e0670767ae0 # oregon-us-west-2c

worker_nodes:
    KeyName: ray-autoscaler_us-west-2

    # 32 CPU, 8 GPU (NVIDIA K80)
    InstanceType: p2.8xlarge 

    # 100G disk size
    BlockDeviceMappings:
    - DeviceName: /dev/sda1
      Ebs:
        VolumeSize: 100
        
    # DL AMI 22, Ray 0.6.6, pytorch 1.1.0, nupic.torch, nupic.research
    ImageId: ami-04b12564224b76fe2 

    # Using "basic_server" seqcurity group
    # It requires VPN when accessing this node from outside the office
    SecurityGroupIds:
        - sg-04ad5dbdb293d7ccb # ray-cluster
    SubnetIds: 
        # - subnet-71025a14  # oregon-default (us-west-2b)
        - subnet-0f5e36e0670767ae0 # oregon-us-west-2c

    # Run workers on spot by default. Comment this out to use on-demand.
    # InstanceMarketOptions:
        # MarketType: spot

# local folders to rsync 
file_mounts: {
    "~/nta/nupic.torch": "~/nta/nupic.torch",
    "~/nta/nupic.research": "~/nta/nupic.research",
}

initialization_commands: []

setup_commands: []
    # Install nupic.torch and nupic.research from the synched folders
    # Only needed when changing the "requirements.txt"
    # - cd ~/nta/nupic.torch && python setup.py develop
    # - cd ~/nta/nupic.research && python setup.py develop
    #
    # Customize your cluster here. The following commands will run on every node
    # - echo "Hello $USER"


head_setup_commands: []

worker_setup_commands: []

head_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --head --redis-port=6379 --object-manager-port=8076 --include-webui --autoscaling-config=~/ray_bootstrap_config.yaml

worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --redis-address=$RAY_HEAD_IP:6379 --object-manager-port=8076
