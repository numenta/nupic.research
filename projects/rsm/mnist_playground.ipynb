{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "import torch\n",
    "from torchnlp.samplers import BPTTBatchSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from rsm_samplers import MNISTSequenceSampler, ptb_pred_sequence_collate\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from importlib import reload \n",
    "from torch.utils.data import Sampler, BatchSampler\n",
    "import rsm\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from functools import reduce, partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(_repackage_hidden(v) for v in h)\n",
    "\n",
    "def activity_square(vector):\n",
    "    n = len(vector)\n",
    "    side = int(np.sqrt(n))\n",
    "    if side ** 2 < n:\n",
    "        side += 1\n",
    "    square = torch.zeros(side ** 2)\n",
    "    square[:n] = vector\n",
    "    return square.view(side, side)\n",
    "    \n",
    "def plot_col_distrs(distrs, n_labels=10):\n",
    "    col_act_avgs = []\n",
    "    fig, axs = plt.subplots(n_labels, n_labels, dpi=200, \n",
    "                            sharex=True, sharey=True,\n",
    "                            gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "    for i in range(n_labels):\n",
    "        for j in range(n_labels):\n",
    "            ax = axs[i][j]\n",
    "            ax.axis('off')\n",
    "            key = '%d-%d' % (i, j)\n",
    "            if key in distrs:\n",
    "                activity_arr = distrs[key]\n",
    "                ax.set_title(\"%d -> %d\" % (i, j), fontsize=7)\n",
    "                dist = torch.stack(activity_arr)\n",
    "                mean_act = dist.mean(dim=0)\n",
    "                ax.imshow(activity_square(mean_act))\n",
    "            else:\n",
    "                ax.set_axis_off()\n",
    "    plt.show()\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsm_samplers\n",
    "import rsm\n",
    "reload(rsm)\n",
    "reload(rsm_samplers)\n",
    "\n",
    "from torch.utils.data import DataLoader, BatchSampler\n",
    "\n",
    "dataset = datasets.MNIST(\"~/nta/datasets\", download=True,\n",
    "                                               transform=transforms.Compose([\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                               ]),)\n",
    "\n",
    "bs=2\n",
    "m=5\n",
    "k=2\n",
    "n=3\n",
    "SEQ = [[0,1,2,3],[0,3,2,1]]\n",
    "sl = 8\n",
    "d_in = d_out = 28 ** 2\n",
    "sampler = rsm_samplers.MNISTSequenceSampler(dataset, batch_size=bs, sequences=SEQ, randomize_sequences=True)\n",
    "batch_sampler = BatchSampler(sampler, batch_size=sl * bs + 1, drop_last=True)\n",
    "\n",
    "collate_fn = partial(rsm_samplers.pred_sequence_collate, \n",
    "                     bsz=bs,\n",
    "                     seq_length=sl,\n",
    "                    return_inputs=True)\n",
    "loader = DataLoader(dataset,\n",
    "                    batch_sampler=batch_sampler,\n",
    "                    collate_fn=collate_fn)\n",
    "model = rsm.RSMLayer(d_in=d_in, d_out=d_out, m=m, n=n, k=k, visual_debug=False, debug=True)\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "LR = .0005\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seqi', 0, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[-0.2635, -0.2635, -0.2635, -0.2424, -0.2424, -0.2424, -0.8727, -0.8727,\n",
      "         -0.8727,  0.4224,  0.4224,  0.4224, -0.1345, -0.1345, -0.1345],\n",
      "        [ 0.3499,  0.3499,  0.3499, -0.0136, -0.0136, -0.0136,  0.5048,  0.5048,\n",
      "          0.5048,  0.5542,  0.5542,  0.5542,  0.0808,  0.0808,  0.0808]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[-0.0600, -0.2421, -0.1107, -0.2644, -0.3350, -0.1488, -0.9187, -0.6598,\n",
      "         -0.8055,  0.3809,  0.4653,  0.3975, -0.1697, -0.3887, -0.3510],\n",
      "        [ 0.5534,  0.3713,  0.5027, -0.0357, -0.1062,  0.0800,  0.4587,  0.7177,\n",
      "          0.5719,  0.5127,  0.5970,  0.5293,  0.0455, -0.1735, -0.1358]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[1.8587, 1.6766, 1.8081, 1.6543, 1.5838, 1.7699, 1.0000, 1.2590, 1.1132,\n",
      "         2.2997, 2.3840, 2.3163, 1.7490, 1.5301, 1.5677],\n",
      "        [2.4721, 2.2901, 2.4215, 1.8831, 1.8126, 1.9987, 2.3774, 2.6364, 2.4906,\n",
      "         2.4314, 2.5158, 2.4480, 1.9643, 1.7453, 1.7829]],\n",
      "       grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[1.8587, 1.7699, 1.2590, 2.3840, 1.7490],\n",
      "        [2.4721, 1.9987, 2.6364, 2.5158, 1.9643]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[1.8587, 0.0000, 0.0000, 0.0000, 0.0000, 1.7699, 0.0000, 1.2590, 0.0000,\n",
      "         0.0000, 2.3840, 0.0000, 1.7490, 0.0000, 0.0000],\n",
      "        [2.4721, 0.0000, 0.0000, 0.0000, 0.0000, 1.9987, 0.0000, 2.6364, 0.0000,\n",
      "         0.0000, 2.5158, 0.0000, 1.9643, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[1.8587, 1.8587, 1.8587, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         2.3840, 2.3840, 2.3840, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6364, 2.6364, 2.6364,\n",
      "         2.5158, 2.5158, 2.5158, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[-0.2045, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.9900,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.9999,\n",
      "          0.0000,  0.0000,  0.9990,  0.0000,  0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.3610, -0.0924, -0.2471,  ..., -0.1286,  0.7035,  0.1344],\n",
      "        [ 0.7099, -0.2277,  0.1964,  ...,  0.0417,  0.8609,  0.5493]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.9900, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.9999, 0.0000,\n",
      "         0.0000, 0.9990, 0.0000, 0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.9900, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.9999, 0.0000,\n",
      "         0.0000, 0.9990, 0.0000, 0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.3312, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.3345, 0.0000,\n",
      "         0.0000, 0.3342, 0.0000, 0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 1, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[ 0.1701,  0.1701,  0.1701, -2.0378, -2.0378, -2.0378,  0.1223,  0.1223,\n",
      "          0.1223,  0.7220,  0.7220,  0.7220, -0.1212, -0.1212, -0.1212],\n",
      "        [-0.2565, -0.2565, -0.2565, -0.8236, -0.8236, -0.8236, -0.4747, -0.4747,\n",
      "         -0.4747,  0.5309,  0.5309,  0.5309,  0.5300,  0.5300,  0.5300]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 0.4330,  0.1904,  0.3960, -2.0727, -2.1666, -1.9273,  0.1460,  0.2748,\n",
      "          0.1495,  0.7070,  0.8438,  0.7198, -0.1435, -0.3423, -0.2888],\n",
      "        [ 0.0145, -0.2445,  0.0256, -0.8989, -0.9874, -0.7622, -0.3703, -0.3223,\n",
      "         -0.3743,  0.5183,  0.7386,  0.4840,  0.5525,  0.3307,  0.3116]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[3.5996e+00, 3.3571e+00, 3.5626e+00, 1.0939e+00, 1.0000e+00, 1.2393e+00,\n",
      "         3.3126e+00, 3.4414e+00, 3.3161e+00, 3.8736e+00, 4.0288e-02, 3.8865e+00,\n",
      "         3.0231e+00, 2.8243e+00, 2.8778e+00],\n",
      "        [3.1811e+00, 2.9221e+00, 3.1922e+00, 2.2677e+00, 2.1792e+00, 2.4044e+00,\n",
      "         2.7964e+00, 2.6447e-04, 2.7923e+00, 3.6849e+00, 4.0789e-03, 3.6506e+00,\n",
      "         3.7191e+00, 3.4973e+00, 3.4783e+00]], grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[3.5996, 1.2393, 3.4414, 3.8865, 3.0231],\n",
      "        [3.1922, 2.4044, 2.7964, 3.6849, 3.7191]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[3.5996, 0.0000, 0.0000, 0.0000, 0.0000, 1.2393, 0.0000, 3.4414, 0.0000,\n",
      "         0.0000, 0.0000, 3.8865, 3.0231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 3.1922, 0.0000, 0.0000, 2.4044, 2.7964, 0.0000, 0.0000,\n",
      "         3.6849, 0.0000, 0.0000, 3.7191, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[3.5996, 3.5996, 3.5996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         3.8865, 3.8865, 3.8865, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         3.6849, 3.6849, 3.6849, 3.7191, 3.7191, 3.7191]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[1.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.6464, -0.3427, -0.0980,  ..., -0.0019,  0.2795,  0.4766],\n",
      "        [ 0.2421, -0.4271, -0.0281,  ..., -0.3588,  1.0099,  0.4397]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[1.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4950, 1.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 0.5000, -0.0000,\n",
      "         1.0000, 0.4995, 0.0000, 1.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[1.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4950, 1.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 0.5000, -0.0000,\n",
      "         1.0000, 0.4995, 0.0000, 1.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.1820, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0901, 0.1820, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 0.0910, -0.0000,\n",
      "         0.1820, 0.0909, 0.0000, 0.1820, 0.0000, 0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 2, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[-0.0834, -0.0834, -0.0834,  0.3774,  0.3774,  0.3774,  0.0994,  0.0994,\n",
      "          0.0994, -0.5306, -0.5306, -0.5306, -0.1906, -0.1906, -0.1906],\n",
      "        [ 0.4413,  0.4413,  0.4413, -0.1970, -0.1970, -0.1970,  0.4954,  0.4954,\n",
      "          0.4954,  0.5332,  0.5332,  0.5332, -0.3010, -0.3010, -0.3010]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 0.0758, -0.0811,  0.1169,  0.3421,  0.2619,  0.4909,  0.0327,  0.2818,\n",
      "          0.1906, -0.5806, -0.4221, -0.4904, -0.2033, -0.4269, -0.3946],\n",
      "        [ 0.6709,  0.4404,  0.6116, -0.2620, -0.3548, -0.0652,  0.4839,  0.6874,\n",
      "          0.5079,  0.5623,  0.6385,  0.4587, -0.3224, -0.5207, -0.5206]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[4.4429e-05, 1.4995e+00, 1.6975e+00, 1.9227e+00, 1.8425e+00, 2.0715e+00,\n",
      "         1.6133e+00, 1.8624e+00, 1.7712e+00, 1.0000e+00, 5.8510e-01, 0.0000e+00,\n",
      "         1.3773e+00, 1.1538e+00, 1.1861e+00],\n",
      "        [2.2515e+00, 2.0210e+00, 2.1922e+00, 1.3186e+00, 1.2258e+00, 1.5154e+00,\n",
      "         2.0646e+00, 1.1341e+00, 2.0885e+00, 3.3209e-06, 1.1107e+00, 2.0394e+00,\n",
      "         5.9995e-07, 1.0600e+00, 1.0600e+00]], grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[1.6975, 2.0715, 1.8624, 1.0000, 1.3773],\n",
      "        [2.2515, 1.5154, 2.0885, 2.0394, 1.0600]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[0.0000, 0.0000, 1.6975, 0.0000, 0.0000, 2.0715, 0.0000, 1.8624, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 1.3773, 0.0000, 0.0000],\n",
      "        [2.2515, 0.0000, 0.0000, 0.0000, 0.0000, 1.5154, 0.0000, 0.0000, 2.0885,\n",
      "         0.0000, 0.0000, 2.0394, 0.0000, 0.0000, 1.0600]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[0.0000, 0.0000, 0.0000, 2.0715, 2.0715, 2.0715, 1.8624, 1.8624, 1.8624,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [2.2515, 2.2515, 2.2515, 0.0000, 0.0000, 0.0000, 2.0885, 2.0885, 2.0885,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.9708, 0.0000, 0.7519, 0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9978, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.9765,\n",
      "         0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.5706,  0.3058, -0.2947,  ...,  0.6851,  0.6863,  0.0231],\n",
      "        [ 0.8425, -0.1716,  0.2881,  ...,  0.4925,  0.1521,  0.7959]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.5000, -0.0000, 0.0000, 0.0000, 0.0000, 0.9708, 0.0000, 0.7519, 0.0000,\n",
      "         -0.0000, 0.2475, 0.5000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9978, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.2500, 0.9765,\n",
      "         0.5000, 0.2497, 0.0000, 0.5000, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.5000, -0.0000, 0.0000, 0.0000, 0.0000, 0.9708, 0.0000, 0.7519, 0.0000,\n",
      "         -0.0000, 0.2475, 0.5000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9978, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.2500, 0.9765,\n",
      "         0.5000, 0.2497, 0.0000, 0.5000, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0776, -0.0000, 0.0000, 0.0000, 0.0000, 0.1507, 0.0000, 0.1167, 0.0000,\n",
      "         -0.0000, 0.0384, 0.0776, -0.0000, -0.0000, -0.0000],\n",
      "        [0.1548, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0388, 0.1515,\n",
      "         0.0776, 0.0388, 0.0000, 0.0776, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 3, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[ 1.1949,  1.1949,  1.1949, -1.4440, -1.4440, -1.4440,  0.7497,  0.7497,\n",
      "          0.7497,  0.2876,  0.2876,  0.2876, -0.1978, -0.1978, -0.1978],\n",
      "        [-0.0154, -0.0154, -0.0154, -0.1044, -0.1044, -0.1044,  0.8830,  0.8830,\n",
      "          0.8830, -1.3931, -1.3931, -1.3931,  0.1154,  0.1154,  0.1154]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 1.3699,  1.1721,  1.4083, -1.4948, -1.5829, -1.3215,  0.7292,  0.9743,\n",
      "          0.8697,  0.2702,  0.3505,  0.2660, -0.2177, -0.4530, -0.4596],\n",
      "        [ 0.2115, -0.0269,  0.1470, -0.1451, -0.2393,  0.0559,  0.7960,  1.0417,\n",
      "          0.9093, -1.3894, -1.2875, -1.4307,  0.0994, -0.1719, -0.1374]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[1.9764, 3.7550, 3.9912, 1.0881, 1.0000, 0.0368, 3.3121, 0.8826, 3.4526,\n",
      "         2.8530, 2.2074, 1.4244, 2.3652, 2.1299, 2.1233],\n",
      "        [0.0062, 2.5560, 2.7299, 2.4377, 2.3436, 2.6388, 3.3789, 2.7185, 0.0822,\n",
      "         0.5967, 0.9719, 1.1521, 1.3411, 2.4109, 2.4455]],\n",
      "       grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[3.9912, 1.0881, 3.4526, 2.8530, 2.3652],\n",
      "        [2.7299, 2.6388, 3.3789, 1.1521, 2.4455]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[0.0000, 0.0000, 3.9912, 1.0881, 0.0000, 0.0000, 0.0000, 0.0000, 3.4526,\n",
      "         2.8530, 0.0000, 0.0000, 2.3652, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 2.7299, 0.0000, 0.0000, 2.6388, 3.3789, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.1521, 0.0000, 0.0000, 2.4455]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[3.9912, 3.9912, 3.9912, 0.0000, 0.0000, 0.0000, 3.4526, 3.4526, 3.4526,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [2.7299, 2.7299, 2.7299, 0.0000, 0.0000, 0.0000, 3.3789, 3.3789, 3.3789,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[0.0000, 0.0000, 1.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.7989, -0.0000, -0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.8513, -0.1753,  0.2989,  ...,  0.4968,  0.1548,  0.8064],\n",
      "        [ 0.7942, -0.1256,  0.2690,  ...,  0.4707,  0.2406,  0.7378]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.2500, 0.0000, 1.0000, -0.0000, -0.0000, 0.4854, 0.0000, 0.3759, 1.0000,\n",
      "         0.0000, 0.1237, 0.2500, -0.0000, -0.0000, -0.0000],\n",
      "        [0.4989, -0.0000, 0.7989, -0.0000, -0.0000, 0.0000, 1.0000, 0.1250, 0.4882,\n",
      "         0.2500, 0.1249, -0.0000, 0.2500, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.2500, 0.0000, 1.0000, -0.0000, -0.0000, 0.4854, 0.0000, 0.3759, 1.0000,\n",
      "         0.0000, 0.1237, 0.2500, -0.0000, -0.0000, -0.0000],\n",
      "        [0.4989, -0.0000, 0.7989, -0.0000, -0.0000, 0.0000, 1.0000, 0.1250, 0.4882,\n",
      "         0.2500, 0.1249, -0.0000, 0.2500, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0356, 0.0000, 0.1424, -0.0000, -0.0000, 0.0691, 0.0000, 0.0535, 0.1424,\n",
      "         0.0000, 0.0176, 0.0356, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0711, -0.0000, 0.1138, -0.0000, -0.0000, 0.0000, 0.1424, 0.0178, 0.0695,\n",
      "         0.0356, 0.0178, -0.0000, 0.0356, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 4, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[-0.0239, -0.0239, -0.0239,  0.1718,  0.1718,  0.1718, -0.0462, -0.0462,\n",
      "         -0.0462, -0.0763, -0.0763, -0.0763, -0.2282, -0.2282, -0.2282],\n",
      "        [ 0.4881,  0.4881,  0.4881, -0.6152, -0.6152, -0.6152,  0.4449,  0.4449,\n",
      "          0.4449,  0.2434,  0.2434,  0.2434, -0.0468, -0.0468, -0.0468]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 0.1853, -0.0674,  0.1605,  0.1914,  0.0440,  0.3060, -0.0615,  0.1211,\n",
      "          0.0048, -0.0642, -0.0104, -0.1291, -0.2390, -0.4972, -0.4763],\n",
      "        [ 0.7036,  0.4582,  0.6661, -0.6030, -0.7018, -0.4665,  0.4130,  0.5864,\n",
      "          0.4738,  0.2693,  0.3259,  0.1906, -0.1142, -0.2995, -0.2703]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[1.4153, 1.6344, 0.0000, 1.8931, 1.7457, 1.0332, 1.6402, 1.1376, 0.0000,\n",
      "         1.6375, 1.4821, 1.1795, 1.4627, 1.2046, 1.2255],\n",
      "        [1.2053, 2.1600, 0.4761, 1.0988, 1.0000, 1.2353, 0.0000, 2.0022, 1.1134,\n",
      "         1.4783, 1.7745, 1.8924, 1.1907, 1.4023, 1.4315]],\n",
      "       grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[1.6344, 1.8931, 1.6402, 1.6375, 1.4627],\n",
      "        [2.1600, 1.2353, 2.0022, 1.8924, 1.4315]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[0.0000, 1.6344, 0.0000, 1.8931, 0.0000, 0.0000, 1.6402, 0.0000, 0.0000,\n",
      "         1.6375, 0.0000, 0.0000, 1.4627, 0.0000, 0.0000],\n",
      "        [0.0000, 2.1600, 0.0000, 0.0000, 0.0000, 1.2353, 0.0000, 2.0022, 0.0000,\n",
      "         0.0000, 0.0000, 1.8924, 0.0000, 0.0000, 1.4315]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[0.0000, 0.0000, 0.0000, 1.8931, 1.8931, 1.8931, 1.6402, 1.6402, 1.6402,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [2.1600, 2.1600, 2.1600, 0.0000, 0.0000, 0.0000, 2.0022, 2.0022, 2.0022,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[ 0.0000, -0.0000,  0.0000,  0.5953,  0.0000,  0.0000, -0.1640,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.9726,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.9820,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.2746,  0.3278, -0.4983,  ...,  0.4151,  0.5145, -0.1571],\n",
      "        [ 0.8373, -0.1661,  0.2868,  ...,  0.4902,  0.1637,  0.7896]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.1250, -0.0000, 0.5000, 0.5953, 0.0000, 0.2427, 0.0000, 0.1880, 0.5000,\n",
      "         -0.0000, 0.0619, 0.1250, -0.0000, -0.0000, -0.0000],\n",
      "        [0.2494, 0.9726, 0.3995, -0.0000, -0.0000, -0.0000, 0.5000, 0.9820, 0.2441,\n",
      "         0.1250, 0.0624, 0.0000, 0.1250, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.1250, -0.0000, 0.5000, 0.5953, 0.0000, 0.2427, 0.0000, 0.1880, 0.5000,\n",
      "         -0.0000, 0.0619, 0.1250, -0.0000, -0.0000, -0.0000],\n",
      "        [0.2494, 0.9726, 0.3995, -0.0000, -0.0000, -0.0000, 0.5000, 0.9820, 0.2441,\n",
      "         0.1250, 0.0624, 0.0000, 0.1250, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0208, -0.0000, 0.0834, 0.0993, 0.0000, 0.0405, 0.0000, 0.0313, 0.0834,\n",
      "         -0.0000, 0.0103, 0.0208, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0416, 0.1622, 0.0666, -0.0000, -0.0000, -0.0000, 0.0834, 0.1637, 0.0407,\n",
      "         0.0208, 0.0104, 0.0000, 0.0208, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 5, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[ 0.4903,  0.4903,  0.4903, -0.4643, -0.4643, -0.4643, -0.2084, -0.2084,\n",
      "         -0.2084,  0.1057,  0.1057,  0.1057, -0.7464, -0.7464, -0.7464],\n",
      "        [ 0.3166,  0.3166,  0.3166, -0.5823, -0.5823, -0.5823,  0.7310,  0.7310,\n",
      "          0.7310, -0.6234, -0.6234, -0.6234,  0.0919,  0.0919,  0.0919]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 0.6748,  0.4679,  0.6551, -0.4709, -0.5918, -0.3272, -0.2499, -0.0061,\n",
      "         -0.1729,  0.0722,  0.1429,  0.0789, -0.7607, -0.9920, -0.9715],\n",
      "        [ 0.5589,  0.3267,  0.5276, -0.6236, -0.7021, -0.4731,  0.7004,  0.9283,\n",
      "          0.7844, -0.6409, -0.4838, -0.6444,  0.0257, -0.1749, -0.1205]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[2.3334, 2.4599, 1.3235, 0.6155, 1.4001, 1.2607, 1.7420, 1.6125, 0.9095,\n",
      "         2.0641, 2.0028, 1.8120, 1.2313, 1.0000, 1.0205],\n",
      "        [1.9146, 0.0636, 1.5131, 1.3684, 1.2899, 1.5189, 1.3462, 0.0526, 2.0986,\n",
      "         1.1822, 1.4140, 1.3476, 1.7654, 1.8171, 1.8715]],\n",
      "       grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[2.4599, 1.4001, 1.7420, 2.0641, 1.2313],\n",
      "        [1.9146, 1.5189, 2.0986, 1.4140, 1.8715]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[0.0000, 2.4599, 0.0000, 0.0000, 1.4001, 0.0000, 1.7420, 0.0000, 0.0000,\n",
      "         2.0641, 0.0000, 0.0000, 1.2313, 0.0000, 0.0000],\n",
      "        [1.9146, 0.0000, 0.0000, 0.0000, 0.0000, 1.5189, 0.0000, 0.0000, 2.0986,\n",
      "         0.0000, 1.4140, 0.0000, 0.0000, 0.0000, 1.8715]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[2.4599, 2.4599, 2.4599, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         2.0641, 2.0641, 2.0641, 0.0000, 0.0000, 0.0000],\n",
      "        [1.9146, 1.9146, 1.9146, 0.0000, 0.0000, 0.0000, 2.0986, 2.0986, 2.0986,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[0.0000, 0.9931, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.2982, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9673, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.9980,\n",
      "         -0.0000, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.5443, -0.1304, -0.1315,  ...,  0.2257,  0.0862,  0.4151],\n",
      "        [ 0.8414, -0.1670,  0.2931,  ...,  0.4923,  0.1684,  0.7944]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.0625, 0.9931, 0.2500, 0.2977, -0.0000, 0.1214, -0.0000, 0.0940, 0.2500,\n",
      "         0.2982, 0.0309, 0.0625, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9673, 0.4863, 0.1997, -0.0000, -0.0000, -0.0000, 0.2500, 0.4910, 0.9980,\n",
      "         0.0625, 0.0312, -0.0000, 0.0625, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.0625, 0.9931, 0.2500, 0.2977, -0.0000, 0.1214, -0.0000, 0.0940, 0.2500,\n",
      "         0.2982, 0.0309, 0.0625, -0.0000, -0.0000, -0.0000],\n",
      "        [0.9673, 0.4863, 0.1997, -0.0000, -0.0000, -0.0000, 0.2500, 0.4910, 0.9980,\n",
      "         0.0625, 0.0312, -0.0000, 0.0625, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0104, 0.1653, 0.0416, 0.0495, -0.0000, 0.0202, -0.0000, 0.0156, 0.0416,\n",
      "         0.0496, 0.0051, 0.0104, -0.0000, -0.0000, -0.0000],\n",
      "        [0.1610, 0.0809, 0.0332, -0.0000, -0.0000, -0.0000, 0.0416, 0.0817, 0.1661,\n",
      "         0.0104, 0.0052, -0.0000, 0.0104, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 6, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[-1.0451, -1.0451, -1.0451,  0.0031,  0.0031,  0.0031, -0.2485, -0.2485,\n",
      "         -0.2485, -1.0427, -1.0427, -1.0427,  0.1441,  0.1441,  0.1441],\n",
      "        [ 0.6367,  0.6367,  0.6367, -0.4710, -0.4710, -0.4710,  0.1801,  0.1801,\n",
      "          0.1801,  0.1401,  0.1401,  0.1401,  0.1719,  0.1719,  0.1719]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[-0.8115, -1.0227, -0.8712, -0.0320, -0.1287,  0.1313, -0.3126, -0.0258,\n",
      "         -0.2307, -1.0846, -0.9702, -1.0363,  0.0733, -0.1220, -0.0508],\n",
      "        [ 0.8722,  0.6305,  0.8211, -0.5008, -0.5924, -0.3232,  0.0904,  0.3421,\n",
      "          0.2239,  0.1253,  0.2656,  0.1243,  0.1352, -0.1338, -0.0749]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[1.1936, 0.0074, 0.9101, 1.4416, 1.9559, 1.9470, 1.7720, 1.8653, 1.3904,\n",
      "         0.7018, 1.0799, 0.9828, 2.1579, 1.9626, 2.0338],\n",
      "        [0.0967, 1.3948, 2.3253, 1.5838, 1.4923, 1.7614, 1.6313, 1.2352, 0.0046,\n",
      "         2.0718, 2.2768, 2.2089, 2.0811, 1.9508, 2.0097]],\n",
      "       grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[1.1936, 1.9559, 1.8653, 1.0799, 2.1579],\n",
      "        [2.3253, 1.7614, 1.6313, 2.2768, 2.0811]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[1.1936, 0.0000, 0.0000, 0.0000, 1.9559, 0.0000, 0.0000, 1.8653, 0.0000,\n",
      "         0.0000, 1.0799, 0.0000, 2.1579, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 2.3253, 0.0000, 0.0000, 1.7614, 1.6313, 0.0000, 0.0000,\n",
      "         0.0000, 2.2768, 0.0000, 2.0811, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[0.0000, 0.0000, 0.0000, 1.9559, 1.9559, 1.9559, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 2.1579, 2.1579, 2.1579],\n",
      "        [2.3253, 2.3253, 2.3253, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         2.2768, 2.2768, 2.2768, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[-0.0000, -0.0000, -0.0000, -0.0000, -0.4560,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.3285, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.9997, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.8802,  0.0000,  0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.1802,  0.0956, -0.2211,  ...,  0.1191,  0.5264,  0.1510],\n",
      "        [ 0.6292, -0.3067, -0.1035,  ...,  0.0371,  0.2461,  0.4664]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.0312, 0.4965, 0.1250, 0.1488, -0.0000, 0.0607, -0.0000, 0.0470, 0.1250,\n",
      "         0.1491, 0.0155, 0.0312, 0.3285, -0.0000, -0.0000],\n",
      "        [0.4837, 0.2431, 0.9997, -0.0000, -0.0000, -0.0000, 0.1250, 0.2455, 0.4990,\n",
      "         0.0312, 0.8802, 0.0000, 0.0312, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.0312, 0.4965, 0.1250, 0.1488, -0.0000, 0.0607, -0.0000, 0.0470, 0.1250,\n",
      "         0.1491, 0.0155, 0.0312, 0.3285, -0.0000, -0.0000],\n",
      "        [0.4837, 0.2431, 0.9997, -0.0000, -0.0000, -0.0000, 0.1250, 0.2455, 0.4990,\n",
      "         0.0312, 0.8802, 0.0000, 0.0312, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.0061, 0.0974, 0.0245, 0.0292, -0.0000, 0.0119, -0.0000, 0.0092, 0.0245,\n",
      "         0.0293, 0.0030, 0.0061, 0.0644, -0.0000, -0.0000],\n",
      "        [0.0949, 0.0477, 0.1961, -0.0000, -0.0000, -0.0000, 0.0245, 0.0482, 0.0979,\n",
      "         0.0061, 0.1727, 0.0000, 0.0061, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['seqi', 7, '-', <class 'int'>]\n",
      "['x_a_row', tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]), torch.Size([2, 784]), torch.float32]\n",
      "['z_a', tensor([[ 0.5019,  0.5019,  0.5019, -0.1537, -0.1537, -0.1537, -0.8280, -0.8280,\n",
      "         -0.8280,  0.6174,  0.6174,  0.6174,  0.0074,  0.0074,  0.0074],\n",
      "        [ 0.4681,  0.4681,  0.4681, -0.5750, -0.5750, -0.5750, -0.4295, -0.4295,\n",
      "         -0.4295,  1.1218,  1.1218,  1.1218, -0.3511, -0.3511, -0.3511]],\n",
      "       grad_fn=<IndexSelectBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['sigma', tensor([[ 0.7118,  0.5169,  0.6651, -0.1942, -0.2771, -0.0320, -0.8996, -0.5961,\n",
      "         -0.8021,  0.5818,  0.6874,  0.6076, -0.0329, -0.2567, -0.1890],\n",
      "        [ 0.7123,  0.4320,  0.6698, -0.5845, -0.6767, -0.4216, -0.4234, -0.2990,\n",
      "         -0.4338,  1.1417,  1.2402,  1.0880, -0.3991, -0.5971, -0.5597]],\n",
      "       grad_fn=<AddBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['pi', tensor([[2.5299e+00, 1.2166e+00, 2.2441e+00, 1.4516e+00, 1.6226e+00, 1.7543e+00,\n",
      "         1.0000e+00, 1.2423e+00, 9.6033e-01, 2.1114e+00, 2.5470e+00, 2.4289e+00,\n",
      "         1.2535e+00, 1.6429e+00, 1.7107e+00],\n",
      "        [1.3487e+00, 1.7647e+00, 7.1536e-04, 1.3152e+00, 1.2229e+00, 1.4780e+00,\n",
      "         1.2917e+00, 1.2077e+00, 7.3436e-01, 2.9463e+00, 3.7615e-01, 2.9876e+00,\n",
      "         1.4536e+00, 1.3026e+00, 1.3400e+00]], grad_fn=<MulBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "['lambda_i', tensor([[2.5299, 1.7543, 1.2423, 2.5470, 1.7107],\n",
      "        [1.7647, 1.4780, 1.2917, 2.9876, 1.4536]], grad_fn=<MaxBackward0>), torch.Size([2, 5]), torch.float32]\n",
      "['M_pi', tensor([[2.5299, 0.0000, 0.0000, 0.0000, 0.0000, 1.7543, 0.0000, 1.2423, 0.0000,\n",
      "         0.0000, 2.5470, 0.0000, 0.0000, 0.0000, 1.7107],\n",
      "        [0.0000, 1.7647, 0.0000, 0.0000, 0.0000, 1.4780, 1.2917, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 2.9876, 1.4536, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['M_lambda', tensor([[2.5299, 2.5299, 2.5299, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         2.5470, 2.5470, 2.5470, 0.0000, 0.0000, 0.0000],\n",
      "        [1.7647, 1.7647, 1.7647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         2.9876, 2.9876, 2.9876, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['y', tensor([[0.9998, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.9997, 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0000, 0.8730, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.0000, 1.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<TanhBackward>), torch.Size([2, 15]), torch.float32]\n",
      "['x_a_next', tensor([[ 0.6463, -0.3425, -0.0980,  ..., -0.0018,  0.2795,  0.4765],\n",
      "        [ 0.6103, -0.3113, -0.1168,  ..., -0.0184,  0.3337,  0.4332]],\n",
      "       grad_fn=<AddmmBackward>), torch.Size([2, 784]), torch.float32]\n",
      "['phi', tensor([[0.9998, 0.2483, 0.0625, 0.0744, -0.0000, 0.0303, -0.0000, 0.0235, 0.0625,\n",
      "         0.0746, 0.9997, 0.0156, 0.1642, -0.0000, -0.0000],\n",
      "        [0.2418, 0.8730, 0.4999, -0.0000, -0.0000, -0.0000, 0.0625, 0.1227, 0.2495,\n",
      "         0.0156, 0.4401, 1.0000, 0.0156, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['psi', tensor([[0.9998, 0.2483, 0.0625, 0.0744, -0.0000, 0.0303, -0.0000, 0.0235, 0.0625,\n",
      "         0.0746, 0.9997, 0.0156, 0.1642, -0.0000, -0.0000],\n",
      "        [0.2418, 0.8730, 0.4999, -0.0000, -0.0000, -0.0000, 0.0625, 0.1227, 0.2495,\n",
      "         0.0156, 0.4401, 1.0000, 0.0156, -0.0000, -0.0000]],\n",
      "       grad_fn=<MaxBackward2>), torch.Size([2, 15]), torch.float32]\n",
      "['x_b', tensor([[0.1593, 0.0396, 0.0100, 0.0119, -0.0000, 0.0048, -0.0000, 0.0037, 0.0100,\n",
      "         0.0119, 0.1593, 0.0025, 0.0262, -0.0000, -0.0000],\n",
      "        [0.0385, 0.1391, 0.0796, -0.0000, -0.0000, -0.0000, 0.0100, 0.0196, 0.0398,\n",
      "         0.0025, 0.0701, 0.1593, 0.0025, -0.0000, -0.0000]],\n",
      "       grad_fn=<DivBackward0>), torch.Size([2, 15]), torch.float32]\n",
      "0 loss 1.34476900100708\n"
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(bs)\n",
    "\n",
    "MAX_BATCHES = 1\n",
    "CLASSES = 10\n",
    "PLOT_EACH = 10\n",
    "EPOCHS = 1\n",
    "\n",
    "condtl_column_dists = {}  # 'digit-digit' -> list of distribution arrays\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for i, (data, targets, pred_targets, input_labels) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden, x_bs = model(data, hidden)\n",
    "        x_b, phi, psi = hidden\n",
    "        for x_b_batch, label_batch, target_batch in zip(x_bs, input_labels, pred_targets):\n",
    "            for _x_b, label, target in zip(x_b_batch, label_batch, target_batch):\n",
    "                digit = label.item()\n",
    "                next_digit = target.item()\n",
    "                col_activity = _x_b.detach().view(m, -1).max(dim=1).values\n",
    "                key = \"%d-%d\" % (digit, next_digit)\n",
    "                if key not in condtl_column_dists:\n",
    "                    condtl_column_dists[key] = []\n",
    "                condtl_column_dists[key].append(col_activity)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        hidden = _repackage_hidden(hidden)\n",
    "\n",
    "        if i >= MAX_BATCHES - 1:\n",
    "            break\n",
    "\n",
    "    print(epoch, 'loss', total_loss / (i+1))\n",
    "#     condtl_column_dists = plot_col_distrs(condtl_column_dists, n_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 12).reshape(2, 4, 3)\n",
    "print(a)\n",
    "values, indices = torch.topk(a, 2)\n",
    "print(indices)\n",
    "arr = a.new_zeros(a.size())  # Zeros, conserve device\n",
    "arr.scatter_(2, indices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(corpus.train), len(corpus.valid), len(corpus.test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "batches = len(corpus.train) / batch_size\n",
    "0.25 * batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def topk_mask(a, k, dim=0, do_softmax=False):\n",
    "    \"\"\"\n",
    "    Return a 1 for the top b elements in the last dim of a, 0 otherwise\n",
    "    \"\"\"\n",
    "    if do_softmax:\n",
    "        return softmax(a)\n",
    "    else:\n",
    "        values, indices = torch.topk(a, k)\n",
    "    arr = a.new_zeros(a.size())  # Zeros, conserve device\n",
    "    arr.scatter_(dim, indices, 1)\n",
    "    return arr\n",
    "\n",
    "a = torch.randn((3, 4))\n",
    "print(a)\n",
    "topk_mask(a, 1, dim=1, do_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LocalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, local_features, kernel_size, stride=1, bias=True):\n",
    "        super(LocalLinear, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        fold_num = (in_features - self.kernel_size) // self.stride + 1\n",
    "        self.lc = nn.ModuleList([deepcopy(nn.Linear(kernel_size, local_features, bias=bias))\n",
    "                                 for _ in range(fold_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.stride)\n",
    "        fold_num = x.shape[1]\n",
    "        x = torch.cat([self.lc[i](x[:, i, :]) for i in range(fold_num)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveDendriteLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Local layer for active dendrites. Similar to a non-shared weight version of a \n",
    "    2D Conv layer.\n",
    "    \n",
    "    Note that dendrites are fully connected to input, local layer used only for connecting\n",
    "    neurons and their dendrites\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_cells=50, n_dendrites=3):\n",
    "        super(ActiveDendriteLayer, self).__init__()\n",
    "        self.n_cells = n_cells\n",
    "        self.n_dendrites = n_dendrites\n",
    "        \n",
    "        total_dendrites = n_dendrites * n_cells\n",
    "        self.linear_dend = nn.Linear(input_dim, total_dendrites)\n",
    "        self.linear_neuron = LocalLinear(total_dendrites, 1, n_dendrites, stride=n_dendrites)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ActiveDendriteLayer neur=%d, dend per neuron=%d\" % (self.n_cells, self.n_dendrites)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_dend(x))\n",
    "        x = self.linear_neuron(x)\n",
    "        return x\n",
    " \n",
    "x = torch.randn(1, 5)\n",
    "print(x)\n",
    "adl = ActiveDendriteLayer(5, 4, 2)\n",
    "print(adl(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 3)\n",
    "x[:, -2:] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1001001010010101101101011010'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BitwiseWordEmbedding(object):\n",
    "\n",
    "    def __init__(self, vocab_size=10000, dim=28):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dict = {}\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        for i in range(self.vocab_size):\n",
    "            self.embedding_dict[i] = self.embed(i)\n",
    "\n",
    "    def embed(self, i):\n",
    "        first = \"{0:b}\".format(i).zfill(self.dim // 2)\n",
    "        return first + self.inverse(first)\n",
    "\n",
    "    def inverse(self, binstr):\n",
    "        return ''.join('1' if x == '0' else '0' for x in binstr)\n",
    "\n",
    "bwe = BitwiseWordEmbedding()\n",
    "\n",
    "bwe.embed(9381)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 432, 3)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload \n",
    "import viz_util\n",
    "reload(viz_util)\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from io import BytesIO\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ax, fig = viz_util.plot_confusion_matrix(np.array([1,2,3]), np.array([1,2,0]), ['0', '1', '2', '3'])\n",
    "\n",
    "img = viz_util.fig2img(fig)\n",
    "\n",
    "print(img.shape)\n",
    "plt.imsave('test.png', img, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgordon/miniconda3/envs/standard/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [2., 2., 2.],\n",
       "        [3., 3., 3.],\n",
       "        [4., 4., 4.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 5).expand((3, 6)).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2177, 0.5582, 0.1910],\n",
      "         [0.0793, 0.9190, 0.8308],\n",
      "         [0.4444, 0.2490, 0.6382],\n",
      "         [0.4885, 0.0823, 0.4100],\n",
      "         [0.4710, 0.3245, 0.6746]],\n",
      "\n",
      "        [[0.0987, 0.9523, 0.1822],\n",
      "         [0.0910, 0.5141, 0.2099],\n",
      "         [0.9616, 0.5538, 0.5057],\n",
      "         [0.1261, 0.0208, 0.4588],\n",
      "         [0.0423, 0.1273, 0.3052]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0],\n",
       "         [0, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         [1, 0, 1],\n",
       "         [1, 0, 1]],\n",
       "\n",
       "        [[0, 1, 1],\n",
       "         [0, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 0, 1],\n",
       "         [0, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nupic.torch.functions import KWinners\n",
    "\n",
    "kw = KWinners.apply\n",
    "\n",
    "bsz = 2\n",
    "m = 5\n",
    "n = 3\n",
    "\n",
    "k = 2\n",
    "\n",
    "a = torch.rand(bsz, m, n)\n",
    "print(a)\n",
    "kw(a.view(bsz * m, n), 0, k, 0).view(bsz, m, n) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2309,  0.2413,  0.0531,  0.1007, -0.1583],\n",
      "        [ 0.1270,  0.2542, -0.0928,  0.0849, -0.4191],\n",
      "        [ 0.2453,  0.2738,  0.1485,  0.0858, -0.2552]],\n",
      "       grad_fn=<AddmmBackward>) torch.Size([3, 5])\n",
      "tensor([[ 0.2309,  0.2309,  0.2309,  0.2309,  0.2413,  0.2413,  0.2413,  0.2413,\n",
      "          0.0531,  0.0531,  0.0531,  0.0531,  0.1007,  0.1007,  0.1007,  0.1007,\n",
      "         -0.1583, -0.1583, -0.1583, -0.1583],\n",
      "        [ 0.1270,  0.1270,  0.1270,  0.1270,  0.2542,  0.2542,  0.2542,  0.2542,\n",
      "         -0.0928, -0.0928, -0.0928, -0.0928,  0.0849,  0.0849,  0.0849,  0.0849,\n",
      "         -0.4191, -0.4191, -0.4191, -0.4191],\n",
      "        [ 0.2453,  0.2453,  0.2453,  0.2453,  0.2738,  0.2738,  0.2738,  0.2738,\n",
      "          0.1485,  0.1485,  0.1485,  0.1485,  0.0858,  0.0858,  0.0858,  0.0858,\n",
      "         -0.2552, -0.2552, -0.2552, -0.2552]], grad_fn=<IndexSelectBackward>)\n",
      "tensor([[ 0.1270,  0.1270,  0.1270,  0.1270],\n",
      "        [ 0.2542,  0.2542,  0.2542,  0.2542],\n",
      "        [-0.0928, -0.0928, -0.0928, -0.0928],\n",
      "        [ 0.0849,  0.0849,  0.0849,  0.0849],\n",
      "        [-0.4191, -0.4191, -0.4191, -0.4191]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "sl = 2\n",
    "bsz = 3\n",
    "m = 5\n",
    "n = 4\n",
    "d_in = 10\n",
    "\n",
    "x = torch.rand((sl, bsz, d_in))\n",
    "\n",
    "x_a = x[0, :]  # first item\n",
    "\n",
    "A = torch.nn.Linear(d_in, m)\n",
    "\n",
    "u = A(x_a)\n",
    "print(u, u.size())\n",
    "z = u.repeat_interleave(n, 1)\n",
    "\n",
    "print(z)\n",
    "\n",
    "first_z_batch = z[1]\n",
    "print(first_z_batch.view(m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import rsm_k_winners\n",
    "reload(rsm_k_winners)\n",
    "\n",
    "def run_kwin(size=50, scatter=True):\n",
    "    return rsm_k_winners.KWinners.apply(torch.rand(size, size), 0, 10, 0, scatter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4548, 0.8208, 0.3500,  ..., 0.0000, 0.0000, 0.2274],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.8647, 0.0963,  ..., 0.7562, 0.0000, 0.6054],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3596, 0.8424, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.4383, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6986]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_kwin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scatter 0.9094837090015062\n",
      "no_scatter 10.072980703000212\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "\n",
    "t = Timer(lambda: run_kwin(scatter=True))\n",
    "print('scatter', t.timeit(number=10000))\n",
    "\n",
    "t = Timer(lambda: run_kwin(scatter=False))\n",
    "print('no_scatter', t.timeit(number=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1703, 0.4110, 0.3670],\n",
      "        [0.3609, 0.5750, 0.4785],\n",
      "        [0.5245, 0.6524, 0.4454]])\n",
      "tensor([[2, 1],\n",
      "        [2, 1],\n",
      "        [0, 1]])\n",
      "tensor([[0.0000, 0.4110, 0.3670],\n",
      "        [0.0000, 0.5750, 0.4785],\n",
      "        [0.5245, 0.6524, 0.0000]])\n",
      "tensor([[0.0000, 0.4110, 0.3670],\n",
      "        [0.0000, 0.5750, 0.4785],\n",
      "        [0.5245, 0.6524, 0.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rsm_k_winners\n",
    "reload(rsm_k_winners)\n",
    "\n",
    "a = torch.rand(3, 3)\n",
    "\n",
    "print(a)\n",
    "\n",
    "res_scatter = rsm_k_winners.KWinners.apply(a.clone(), 0, 2, 0, True)\n",
    "res_no_scatter = rsm_k_winners.KWinners.apply(a.clone(), 0, 2, 0, False)\n",
    "\n",
    "print(res_scatter)\n",
    "print(res_no_scatter)\n",
    "bool(torch.all(torch.eq(res_scatter, res_no_scatter)).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
