# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://jgordon/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

experiment = grid
path = ~/nta/results
data_dir = ~/nta/datasets

# Data
dataset = ptb

# Common network parameters
m_groups = 600
n_cells_per_group = 8
k_winners = 20
k_winner_cells = 1
gamma = 0.8
eps = 0.85

model_kind = rsm

cell_winner_softmax = False
activation_fn = tanh

repetitions = 1
momentum = 0.9
save_onnx_graph_at_checkpoint = False

embed_dim = 28  # Binary hash dimension
input_size = (1, 28)
output_size = 28
vocab_size = 10000
optimizer = adam
learning_rate = 0.0005
learning_rate_gamma = 0

gpu_percentage = 1.0
# Or MSELoss
loss_function = MSELoss
checkpoint_freq = 25
# Paper claims best test results at 25 pct of dataset, 4 epochs for train
iterations = 5
batch_size = 300
seq_length=1
batches_in_epoch = 250000
eval_batches_in_epoch = 1000
batch_log_interval = 500
eval_interval = 1  # Each x epochs

predictor_hidden_size=1200
predictor_output_size=%(vocab_size)s

stop = {"stop": 1}
checkpoint_at_end = True

################
[RSM_PTB_Paper]
batch_log_interval = 10000

################
[RSM_PTB_ReLU]
activation_fn = relu

################
[RSM_PTB_2Cells]
k_winner_cells = 2

################
[RSM_PTB_ActiveDend]
active_dendrites = 2

################
[RSM_PTB_ColOutputCell]
# Clamp x cells per group/column to max value prior to k-winners
# Possibly provides a useful column-level representation to memory and
# increases overlap between similar representations?
# Use k_winner_cells > 1 since winners chosen after output cells clamped
k_winner_cells = 2
col_output_cells = 1  

################
[RSM_PTB_Small]
iterations = 4
eval_interval = 1  # Each x epochs
m_groups = 200
n_cells_per_group = 6
k_winners = 25
batch_size = 64
gamma = 0.8
eps = 0.85
batches_in_epoch = 200
eval_batches_in_epoch = 40
batch_log_interval = 25
predictor_hidden_size=300

################
[RSM_PTB_Debug]
iterations = 4
eval_interval = 1  # Each x epochs
m_groups = 50
n_cells_per_group = 3
k_winners = 5
seq_length=1
batch_size = 32
batches_in_epoch = 20
eval_batches_in_epoch = 20
batch_log_interval = 10
predictor_hidden_size=50

################
[RSM_PTB_LSTM]
model_kind = lstm
learning_rate = 4.0
learning_rate_gamma = 0.25
iterations = 40
eval_interval = 1
batch_size = 20
batches_in_epoch = 100000
eval_batches_in_epoch = 100000
seq_length=35
batch_log_interval = 10000
predictor_hidden_size = 0

################
[RSM_PTB_LSTM_Debug]
model_kind = lstm
learning_rate = 4.0
learning_rate_gamma = 0.25
iterations = 40
eval_interval = 1
batch_size = 20
batches_in_epoch = 100
seq_length=25
eval_batches_in_epoch = 20
batch_log_interval = 10
predictor_hidden_size = 0

# TODO: Dropout, L2 reg, forgetting?
