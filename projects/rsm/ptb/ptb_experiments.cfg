# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://jgordon/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

experiment = grid
path = ~/nta/results
data_dir = ~/nta/datasets

# Data
dataset = ptb

# Common network parameters
m_groups = 600
n_cells_per_group = 8
k_winners = 20
k_winner_cells = 1
gamma = 0.8
eps = 0.85

model_kind = rsm

activation_fn = tanh

repetitions = 1
momentum = 0.9
save_onnx_graph_at_checkpoint = False

embed_dim = 28  # Binary hash dimension
input_size = (1, 28)
output_size = 28
vocab_size = 10000
optimizer = adam
learning_rate = 0.0005
pred_learning_rate = 0.0005
learning_rate_gamma = 0
x_b_norm=False
gpu_percentage = 1.0
loss_function = MSELoss
# Paper claims best test results at 25 pct of dataset, 4 epochs for train
iterations = 5
batch_size = 300
eval_batch_size = 10
batches_in_first_epoch=2  # At least 2 to make an optimizer step
batches_in_epoch = 2000  # 250k RSM best on val (quarter of 'epoch')
eval_batches_in_epoch = 8200
batch_log_interval = 500
eval_interval = 3  # Each x epochs

predictor_hidden_size=1200
predictor_output_size=%(vocab_size)s

stop = {"stop": 1}
checkpoint_at_end = True

################
[ORSM_PTB]
instrumentation = False
iterations=200
predictor_hidden_size=0

################
[Flat_PTB_Explore]
pred_l2_reg=0.000001
gpu_percentage=0.5
checkpoint_at_end=False
iterations=33
forget_mu=0.025
input_bias=True
m_groups=4000
k_winners=40
n_cells_per_group=1
k_winner_cells=1
eps=.75
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
do_inhibition=False
boost_strength=1.0
boost_strength_factor=tune.grid_search([0.6, 0.7, 1])
mult_integration=False
weight_sparsity=None
fpartition=None
instrumentation = False
# instr_charts = ["img_preds"]
balance_part_winners=True
predictor_hidden_size=0
eval_interval = 3
pause_learning=False
pause_after_epochs=27

################
[Flat_PTB_Explore_Local]
l2_reg=0.000001
pred_l2_reg=0.000001
predictor_hidden_size=300
gpu_percentage=1.0
checkpoint_at_end=False
iterations=60
forget_mu=0.025
input_bias=True
m_groups=2000
k_winners=40
n_cells_per_group=1
k_winner_cells=1
eps=0.0
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
do_inhibition=False
boost_strength=1.0
boost_strength_factor=0.97
mult_integration=True
weight_sparsity=None
fpartition=0.3
batch_size=30
# instrumentation = True
# instr_charts = ["img_preds"]
balance_part_winners=True

################
[Adj_2L_PTB]
iterations = 200
m_groups=200
n_cells_per_group = 4
eps = [0.0, 0.0]
gamma = 0.0
k_winner_cells=1
boost_strat = col_boosting
x_b_norm = True
n_layers=2
input_bias=True
decode_bias=True
tp_boosting = True
predict_layer = 0
loss_layers = all_layers
feedback_conn = True
instrumentation = True
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext


################
[Flat_2L_PTB_Explore]
pred_l2_reg=0.00001
gpu_percentage = 1.0
checkpoint_at_end=False
iterations = 60
m_groups=3000
k_winners=50
n_cells_per_group = 1
eps = [0.0, 0.0]
gamma = 0.0
forget_mu=0.02
k_winner_cells=1
boost_strat = col_boosting
boost_strength=[1.0, 0.2]
boost_strength_factor=0.9
x_b_norm = True
n_layers=2
input_bias=True
decode_bias=True
do_inhibition=False
mult_integration=False
tp_boosting = False
predict_layer = 0
loss_layers = all_layers
top_lateral_conn = False
feedback_conn = True
instrumentation = True
instr_charts = ['img_memory_snapshot']
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext

################
[Flat_PTB_Snipe2]
checkpoint_at_end=False
gpu_percentage=0.5
iterations=75
forget_mu=0.02
m_groups=4000
k_winners=50
n_cells_per_group=1
k_winner_cells=1
batches_in_epoch=2000
eps=0.85
gamma=0.0
boost_strat=col_boosting
x_b_norm=False
do_inhibition=False
boost_strength=1.0
boost_strength_factor=0.8
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=False


################
[Flat_PTB_Debug]
learning_rate=0.0005
pred_learning_rate=0.0005
pred_l2_reg=0.000001
gpu_percentage=1.0
checkpoint_at_end=False
iterations=50000
batches_in_epoch=3
batches_in_first_epoch=1
batch_size=150
eval_interval=500
forget_mu=0.025
m_groups=3000
k_winners=50
n_cells_per_group=1
k_winner_cells=1
eps=0.5
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
do_inhibition=False
boost_strength=0.3
boost_strength_factor=0.7
mult_integration=False
weight_sparsity=None
fpartition=0.5
instrumentation = True
balance_part_winners=False
mem_gain=1.0


################
[Flat_PTB_Debug2]
learning_rate=0.00025
pred_l2_reg=0.000001
gpu_percentage=1.0
checkpoint_at_end=False
iterations=60
forget_mu=0.025
input_bias=True
m_groups=4000
k_winners=20
n_cells_per_group=1
k_winner_cells=1
eps=0.0
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
do_inhibition=False
boost_strength=1.0
boost_strength_factor=0.85
mult_integration=False
weight_sparsity=None
fpartition=0.3
instrumentation = False
# instr_charts = ["img_preds"]
balance_part_winners=True
predictor_hidden_size=0  # Disabled