# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

[DEFAULT]
# AWS sync
# Uncomment to upload results on S3
upload_dir = "s3://jgordon/ray/results"
sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

experiment = grid
path = ~/nta/results
data_dir = ~/nta/datasets

# Data
dataset = ptb

# Common network parameters
m_groups = 600
n_cells_per_group = 8
k_winners = 20
k_winner_cells = 1
gamma = 0.8
eps = 0.85

model_kind = rsm

activation_fn = tanh

repetitions = 1
momentum = 0.9
save_onnx_graph_at_checkpoint = False

embed_dim = 28  # Binary hash dimension
input_size = (1, 28)
output_size = 28
vocab_size = 10000
optimizer = adam
pred_optimizer = adam
learning_rate = 0.0005
pred_learning_rate = 0.0005
learning_rate_gamma = 0
x_b_norm=False
gpu_percentage = 1.0
loss_function = MSELoss
# Paper claims best test results at 25 pct of dataset, 4 epochs for train
iterations = 5
batch_size = 300
batches_in_first_epoch=2  # At least 2 to make an optimizer step
batches_in_epoch = 2000  # 250k RSM best on val (quarter of 'epoch')
eval_batch_size = 10
eval_batches_in_epoch = 8243  # one pass 10 x 8200 ~ test corpus size
batch_log_interval = 500
eval_interval = 3  # Each x epochs

predictor_hidden_size=1200
predictor_output_size=%(vocab_size)s

stop = {"stop": 1}
checkpoint_at_end = True

################
[ORSM_PTB]
instrumentation = False
iterations=200
predictor_hidden_size=0

################
[Flat_PTB_Ramp]
gpu_percentage=1.0
batches_in_epoch = 1000
pred_l2_reg=0.000001
# l2_reg=0.00000001
dec_l2_reg=0.000001
checkpoint_at_end=False
iterations=152
forget_mu=0.025
input_bias=True
m_groups=1500
k_winners=80
n_cells_per_group=1
k_winner_cells=1
eps=.5
boost_strat=col_boosting
x_b_norm=True
boost_strength=0.5
boost_strength_factor= 0.85 # tune.grid_search([.9, 0.95])
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
instrumentation = True
predictor_hidden_size=1200
eval_interval = 5
#eval_interval_schedule = [(80, 2)]
pause_after_upticks=0
pause_after_epochs=0
pause_min_epoch=45
decode_activation_fn=None
decode_bias=False
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext_e5
# max_decay=0.95
# mem_floor=0.0005
predictor_log_softmax=True
trainable_decay=True
ramping_memory=True
ramping_fn=normal
ramping_learn_vel=True
activation_fn=relu

################
[Flat_PTB_Best]
gpu_percentage=0.5
batches_in_epoch = 1000
pred_l2_reg=0.000001
dec_l2_reg=0.000001
iterations=152
forget_mu=0.025
input_bias=True
m_groups=1500
k_winners=80
n_cells_per_group=1
k_winner_cells=1
eps=.5
boost_strat=col_boosting
x_b_norm=True
boost_strength=0.5
boost_strength_factor= 0.85 # tune.grid_search([.9, 0.95])
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
instrumentation = False
predictor_hidden_size=1200
eval_interval = 5
pause_after_upticks=0
pause_after_epochs=45
pause_min_epoch=0
decode_activation_fn=None
decode_bias=False
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext_e5
max_decay=0.95
mem_floor=0.0005
trainable_decay=True
word_cache_decay=0.99
word_cache_pct=0.05
unif_smoothing=0.01

################
[Flat_PTB_Explore]
gpu_percentage=0.5
batches_in_epoch = 1000
pred_l2_reg=0.000001
dec_l2_reg=0.000001
checkpoint_at_end=False
iterations=152
forget_mu=0.025
input_bias=True
m_groups=1500
k_winners=80
n_cells_per_group=1
k_winner_cells=1
eps=.5
boost_strat=col_boosting
x_b_norm=True
boost_strength=0.5
boost_strength_factor= 0.85 # tune.grid_search([.9, 0.95])
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
instrumentation = False
predictor_hidden_size=1200
eval_interval = 5
pause_after_upticks=0
pause_after_epochs=55
pause_min_epoch=0
decode_activation_fn=None
decode_bias=False
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext_e5
max_decay=0.95
mem_floor=0.0005
trainable_decay=True
word_cache_decay=0.99
word_cache_pct=0.05
unif_smoothing=0.01
batches_in_first_epoch=1000

################
[Flat_PTB_SGD]
gpu_percentage=0.5
optimizer=sgd
pred_optimizer=adam
pred_learning_rate=0.0005
learning_rate=.1
learning_rate_gamma=.2
learning_rate_min=1e-8
batches_in_epoch = 1000
pred_l2_reg=0.000001
dec_l2_reg=0.000001
checkpoint_at_end=False
iterations=152
forget_mu=0.025
input_bias=True
m_groups=1500
k_winners=80
n_cells_per_group=1
k_winner_cells=1
eps=.5
boost_strat=col_boosting
x_b_norm=True
boost_strength=0.5
boost_strength_factor= 0.85
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
instrumentation = False
predictor_hidden_size=1200
eval_interval = 10
#eval_interval_schedule = [(80, 2)]
pause_after_upticks=0
pause_after_epochs=0
pause_min_epoch=0
decode_activation_fn=None
decode_bias=False
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext_e5
trainable_decay=True

################
[Flat_2L_PTB_Explore]
checkpoint_at_end=True
gpu_percentage=0.5
pred_l2_reg=0.000001
batches_in_epoch = 1000
iterations=92
forget_mu=0.025
input_bias=True
m_groups=[1500, 800]
k_winners=[80, 40]
n_cells_per_group=[1, 1]
k_winner_cells=1
eps=[0.5, 0.5]
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
boost_strength=[1.0, 0.2]
boost_strength_factor=0.85
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
predictor_hidden_size=1200
n_layers=2
loss_layers=all_layers
eval_interval = 5
eval_interval_schedule = [(60, 2)]
pause_after_upticks=2
pause_after_epochs=0
pause_min_epoch=60
trainable_decay=True
decode_activation_fn=sigmoid
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext
skip_context=0.0
skip_context_val=False
feedback_conn=True

################
[Adj_PTB_Explore]
gpu_percentage=1.0
decode_from_full_memory=False
batches_in_epoch = 1000
# l2_reg=0.0000001
pred_l2_reg=0.000001
checkpoint_at_end=False
iterations=250
forget_mu=0.025
input_bias=True
m_groups=600
n_cells_per_group=3
k_winners=80
k_winner_cells=1
eps=.5
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
boost_strength=1.0
boost_strength_factor=0.95
mult_integration=False
weight_sparsity=None
fpartition=None
balance_part_winners=True
instrumentation = False
predictor_hidden_size=1200
eval_interval = 10
eval_interval_schedule = [(80, 2)]
pause_after_upticks=1
pause_after_epochs=0
pause_min_epoch=80
trainable_decay=True
decode_activation_fn=sigmoid
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext_e15
col_output_cells=False

################
[Adj_2L_PTB]
iterations = 200
m_groups=200
n_cells_per_group = 4
eps = [0.0, 0.0]
gamma = 0.0
k_winner_cells=1
boost_strat = col_boosting
x_b_norm = True
n_layers=2
input_bias=True
decode_bias=True
tp_boosting = True
predict_layer = 0
loss_layers = all_layers
feedback_conn = True
instrumentation = True
embed_dim=100
input_size = (1, 100)
output_size = %(embed_dim)s
embedding_kind=ptb_fasttext

################
[Flat_PTB_Debug]
pred_l2_reg=0.000001
gpu_percentage=1.0
checkpoint_at_end=False
batch_size=50
batches_in_epoch=3
iterations=150
forget_mu=0.025
input_bias=True
m_groups=200
k_winners=20
n_cells_per_group=1
k_winner_cells=1
eps=0.5
gamma=0.0
boost_strat=col_boosting
x_b_norm=True
boost_strength=0.5
boost_strength_factor=0.85
mult_integration=False
weight_sparsity=None
fpartition=None
instrumentation = False
balance_part_winners=True
predictor_hidden_size=1200
eval_interval = 2
eval_batch_size = 100
eval_batches_in_epoch = 824  # one pass 10 x 8200 ~ test corpus size
pause_after_epochs=25
feedback_conn=False
word_cache_decay=0.6
word_cache_pct=0.05
unif_smoothing=0.05


