{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "import torch\n",
    "from torchnlp.samplers import BPTTBatchSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from rsm_samplers import LangSequenceSampler, language_pred_sequence_collate\n",
    "from ptb_lstm import LSTMModel\n",
    "from lang_util import Corpus\n",
    "import torch.nn.functional as F\n",
    "from importlib import reload \n",
    "import rsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[slice(0, 3, None),\n",
       " slice(185918, 185921, None),\n",
       " slice(371836, 371839, None),\n",
       " slice(557754, 557757, None),\n",
       " slice(743672, 743675, None)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus('/Users/jgordon/nta/datasets/PTB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([3, 2, 10000])\n",
      "targets torch.Size([6])\n",
      "9.043150901794434\n",
      "output torch.Size([3, 2, 10000])\n",
      "targets torch.Size([6])\n",
      "9.00977897644043\n",
      "output torch.Size([3, 2, 10000])\n",
      "targets torch.Size([6])\n",
      "9.214032173156738\n",
      "output torch.Size([3, 2, 10000])\n",
      "targets torch.Size([6])\n",
      "9.105277061462402\n",
      "output torch.Size([3, 2, 10000])\n",
      "targets torch.Size([6])\n",
      "9.204472541809082\n"
     ]
    }
   ],
   "source": [
    "reload(rsm)\n",
    "\n",
    "BS = 2\n",
    "SEQL = 3\n",
    "VS = 10000\n",
    "model = rsm.RSMLayer(d_in=30, d_out=VS, m=4, n=3, k=2, k_winner_cells=2, \n",
    "                        vocab_size=VS, embed_dim=30, debug=False)\n",
    "\n",
    "sampler = LangSequenceSampler(corpus.train, batch_size=BS,\n",
    "                                                seq_length=SEQL,\n",
    "                                                parallel_seq=True)\n",
    "loader = DataLoader(corpus.train,\n",
    "                       batch_sampler=sampler,\n",
    "                       collate_fn=language_pred_sequence_collate)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "hidden = model.init_hidden(BS)\n",
    "\n",
    "for i, (data, targets, _) in enumerate(loader):\n",
    "    model.zero_grad()\n",
    "    output, hidden, _ = model(data, hidden)\n",
    "    print('output', output.size())\n",
    "    print('targets', targets.size())\n",
    "    loss = criterion(output.view(-1, VS), targets)\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "    hidden = _repackage_hidden(hidden)\n",
    "    if i > 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6471, -1.0628,  1.0940],\n",
      "         [-0.3992,  2.3036, -1.0940],\n",
      "         [ 0.8734, -2.1454,  2.0653],\n",
      "         [ 0.7241, -1.7697, -0.2021]],\n",
      "\n",
      "        [[ 1.1691, -0.2936, -0.5580],\n",
      "         [-1.2807, -1.0672,  0.2157],\n",
      "         [ 0.5309,  0.1857, -0.2516],\n",
      "         [ 1.1832, -1.2698,  0.2208]]])\n",
      "tensor([[[2, 0],\n",
      "         [1, 0],\n",
      "         [2, 0],\n",
      "         [0, 2]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [0, 1],\n",
      "         [0, 2]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 0.],\n",
       "         [0., 1., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 0., 1.]]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 12).reshape(2, 4, 3)\n",
    "print(a)\n",
    "values, indices = torch.topk(a, 2)\n",
    "print(indices)\n",
    "arr = a.new_zeros(a.size())  # Zeros, conserve device\n",
    "arr.scatter_(2, indices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[929589, 73760, 82430]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(corpus.train), len(corpus.valid), len(corpus.test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774.6575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 300\n",
    "batches = len(corpus.train) / batch_size\n",
    "0.25 * batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3621,  0.2758, -0.5286,  0.0746],\n",
      "        [ 0.8212,  0.9581, -0.0496, -0.1561],\n",
      "        [ 1.4873, -0.6304,  1.5969,  1.2753]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgordon/miniconda3/envs/standard/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5668, 0.1913, 0.0856, 0.1564],\n",
       "        [0.3400, 0.3898, 0.1423, 0.1279],\n",
       "        [0.3284, 0.0395, 0.3664, 0.2657]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def topk_mask(a, k, dim=0, do_softmax=False):\n",
    "    \"\"\"\n",
    "    Return a 1 for the top b elements in the last dim of a, 0 otherwise\n",
    "    \"\"\"\n",
    "    if do_softmax:\n",
    "        return softmax(a)\n",
    "    else:\n",
    "        values, indices = torch.topk(a, k)\n",
    "    arr = a.new_zeros(a.size())  # Zeros, conserve device\n",
    "    arr.scatter_(dim, indices, 1)\n",
    "    return arr\n",
    "\n",
    "a = torch.randn((3, 4))\n",
    "print(a)\n",
    "topk_mask(a, 1, dim=1, do_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2939,  0.5844,  0.6376, -0.2840, -0.3008]])\n",
      "tensor([[ 0.3429, -0.1966, -0.4056, -0.0240]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LocalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, local_features, kernel_size, stride=1, bias=True):\n",
    "        super(LocalLinear, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        fold_num = (in_features - self.kernel_size) // self.stride + 1\n",
    "        self.lc = nn.ModuleList([deepcopy(nn.Linear(kernel_size, local_features, bias=bias))\n",
    "                                 for _ in range(fold_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.stride)\n",
    "        fold_num = x.shape[1]\n",
    "        x = torch.cat([self.lc[i](x[:, i, :]) for i in range(fold_num)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveDendriteLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Local layer for active dendrites. Similar to a non-shared weight version of a \n",
    "    2D Conv layer.\n",
    "    \n",
    "    Note that dendrites are fully connected to input, local layer used only for connecting\n",
    "    neurons and their dendrites\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_cells=50, n_dendrites=3):\n",
    "        super(ActiveDendriteLayer, self).__init__()\n",
    "        self.n_cells = n_cells\n",
    "        self.n_dendrites = n_dendrites\n",
    "        \n",
    "        total_dendrites = n_dendrites * n_cells\n",
    "        self.linear_dend = nn.Linear(input_dim, total_dendrites)\n",
    "        self.linear_neuron = LocalLinear(total_dendrites, 1, n_dendrites, stride=n_dendrites)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ActiveDendriteLayer neur=%d, dend per neuron=%d\" % (self.n_cells, self.n_dendrites)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_dend(x))\n",
    "        x = self.linear_neuron(x)\n",
    "        return x\n",
    " \n",
    "x = torch.randn(1, 5)\n",
    "print(x)\n",
    "adl = ActiveDendriteLayer(5, 4, 2)\n",
    "print(adl(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1196,  1.0000,  1.0000],\n",
      "        [-0.0664,  1.0000,  1.0000],\n",
      "        [-0.3680,  1.0000,  1.0000],\n",
      "        [ 1.5387,  1.0000,  1.0000],\n",
      "        [ 1.1104,  1.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "x[:, -2:] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
