{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "import torch\n",
    "from lang_util import Corpus\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = penn_treebank_dataset('/Users/jgordon/nta/datasets/PTB', train=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('/Users/jgordon/nta/datasets/PTB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[929589, 73760, 82430]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(corpus.train), len(corpus.valid), len(corpus.test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774.6575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 300\n",
    "batches = len(corpus.train) / batch_size\n",
    "0.25 * batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3621,  0.2758, -0.5286,  0.0746],\n",
      "        [ 0.8212,  0.9581, -0.0496, -0.1561],\n",
      "        [ 1.4873, -0.6304,  1.5969,  1.2753]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgordon/miniconda3/envs/standard/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5668, 0.1913, 0.0856, 0.1564],\n",
       "        [0.3400, 0.3898, 0.1423, 0.1279],\n",
       "        [0.3284, 0.0395, 0.3664, 0.2657]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def topk_mask(a, k, dim=0, do_softmax=False):\n",
    "    \"\"\"\n",
    "    Return a 1 for the top b elements in the last dim of a, 0 otherwise\n",
    "    \"\"\"\n",
    "    if do_softmax:\n",
    "        return softmax(a)\n",
    "    else:\n",
    "        values, indices = torch.topk(a, k)\n",
    "    arr = a.new_zeros(a.size())  # Zeros, conserve device\n",
    "    arr.scatter_(dim, indices, 1)\n",
    "    return arr\n",
    "\n",
    "a = torch.randn((3, 4))\n",
    "print(a)\n",
    "topk_mask(a, 1, dim=1, do_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2939,  0.5844,  0.6376, -0.2840, -0.3008]])\n",
      "tensor([[ 0.3429, -0.1966, -0.4056, -0.0240]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LocalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, local_features, kernel_size, stride=1, bias=True):\n",
    "        super(LocalLinear, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        fold_num = (in_features - self.kernel_size) // self.stride + 1\n",
    "        self.lc = nn.ModuleList([deepcopy(nn.Linear(kernel_size, local_features, bias=bias))\n",
    "                                 for _ in range(fold_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.stride)\n",
    "        fold_num = x.shape[1]\n",
    "        x = torch.cat([self.lc[i](x[:, i, :]) for i in range(fold_num)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveDendriteLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Local layer for active dendrites. Similar to a non-shared weight version of a \n",
    "    2D Conv layer.\n",
    "    \n",
    "    Note that dendrites are fully connected to input, local layer used only for connecting\n",
    "    neurons and their dendrites\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_cells=50, n_dendrites=3):\n",
    "        super(ActiveDendriteLayer, self).__init__()\n",
    "        self.n_cells = n_cells\n",
    "        self.n_dendrites = n_dendrites\n",
    "        \n",
    "        total_dendrites = n_dendrites * n_cells\n",
    "        self.linear_dend = nn.Linear(input_dim, total_dendrites)\n",
    "        self.linear_neuron = LocalLinear(total_dendrites, 1, n_dendrites, stride=n_dendrites)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ActiveDendriteLayer neur=%d, dend per neuron=%d\" % (self.n_cells, self.n_dendrites)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_dend(x))\n",
    "        x = self.linear_neuron(x)\n",
    "        return x\n",
    " \n",
    "x = torch.randn(1, 5)\n",
    "print(x)\n",
    "adl = ActiveDendriteLayer(5, 4, 2)\n",
    "print(adl(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1196,  1.0000,  1.0000],\n",
      "        [-0.0664,  1.0000,  1.0000],\n",
      "        [-0.3680,  1.0000,  1.0000],\n",
      "        [ 1.5387,  1.0000,  1.0000],\n",
      "        [ 1.1104,  1.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "x[:, -2:] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
