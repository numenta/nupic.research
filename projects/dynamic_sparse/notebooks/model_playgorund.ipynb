{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Playground\n",
    "\n",
    "**Various tests and small experiments on toy networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import nupic.research.frameworks.dynamic_sparse.networks.layers as layers\n",
    "reload(layers);\n",
    "import nupic.research.frameworks.dynamic_sparse.networks.layers as networks\n",
    "reload(networks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from nupic.research.frameworks.dynamic_sparse.networks.layers import DSConv2d\n",
    "from nupic.torch.models.sparse_cnn import gsc_sparse_cnn, gsc_super_sparse_cnn, GSCSparseCNN, MNISTSparseCNN\n",
    "from nupic.research.frameworks.dynamic_sparse.networks import mnist_sparse_dscnn, GSCSparseFullCNN, gsc_sparse_dscnn_fullyconv\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()\n",
    "# mnist_scnn = MNISTSparseCNN()\n",
    "gsc_scnn = GSCSparseCNN()\n",
    "# dscnn = mnist_sparse_dscnn({})\n",
    "# gscf = gsc_sparse_dscnn_fullyconv({'prune_methods': [\"none\", \"static\"]}) # GSCSparseFullCNN(cnn_out_channels=(32, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GSCSparseCNN(\n",
       "  (cnn1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (cnn1_batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (cnn1_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn1_kwinner): KWinners2d(channels=64, n=0, percent_on=0.095, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.5, duty_cycle_period=1000)\n",
       "  (cnn2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (cnn2_batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (cnn2_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2_kwinner): KWinners2d(channels=64, n=0, percent_on=0.125, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.5, duty_cycle_period=1000)\n",
       "  (flatten): Flatten()\n",
       "  (linear): SparseWeights(\n",
       "    weight_sparsity=0.4\n",
       "    (module): Linear(in_features=1600, out_features=1000, bias=True)\n",
       "  )\n",
       "  (linear_bn): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (linear_kwinner): KWinners(n=1000, percent_on=0.1, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.5, duty_cycle_period=1000)\n",
       "  (output): Linear(in_features=1000, out_features=12, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet18\n",
    "# resnet18\n",
    "# mnist_scnn\n",
    "gsc_scnn\n",
    "# dscnn\n",
    "# gscf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi <class 'nupic.torch.models.sparse_cnn.GSCSparseCNN'>\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]               0\n",
      "        KWinners2d-3           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-4           [-1, 32, 14, 14]               0\n",
      "      SparseConv2d-5           [-1, 64, 10, 10]          51,264\n",
      "       BatchNorm2d-6           [-1, 64, 10, 10]               0\n",
      "        KWinners2d-7           [-1, 64, 10, 10]               0\n",
      "         MaxPool2d-8             [-1, 64, 5, 5]               0\n",
      "           Flatten-9                 [-1, 1600]               0\n",
      "           Linear-10                   [-1, 12]          19,212\n",
      "       LogSoftmax-11                   [-1, 12]               0\n",
      "================================================================\n",
      "Total params: 71,308\n",
      "Trainable params: 71,308\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.79\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 1.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand(2, 1, 32, 32)\n",
    "gsc_scnn(inp).shape\n",
    "gscf(inp).shape\n",
    "\n",
    "summary(gscf, input_size=(1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with sequentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq0 = torch.nn.Sequential(OrderedDict([('sq1', torch.nn.Sequential(OrderedDict([('cnn1', torch.nn.Conv2d(3, 3, 3))])) )]))\n",
    "sq1 = torch.nn.Sequential(od)\n",
    "sq2 = torch.nn.Sequential(torch.nn.Sequential(od), torch.nn.Conv2d(3, 3, 3))\n",
    "sq3 = torch.nn.Sequential(OrderedDict([('sq1', sq1), ('sq2', sq2)]))\n",
    "sq4 = torch.nn.Sequential(sq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "name\n",
      " Sequential(\n",
      "  (0): Sequential(\n",
      "    (sq1): Sequential(\n",
      "      (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (sq2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[True]\n",
      "name\n",
      "0 Sequential(\n",
      "  (sq1): Sequential(\n",
      "    (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (sq2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "[True, False]\n",
      "name\n",
      "0.sq1 Sequential(\n",
      "  (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "[True, False, False]\n",
      "name\n",
      "0.sq1.cnn1 Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "[True, False]\n",
      "name\n",
      "0.sq2 Sequential(\n",
      "  (0): Sequential(\n",
      "    (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "[True, False, True]\n",
      "name\n",
      "0.sq2.0 Sequential(\n",
      "  (cnn1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "[True, False, True]\n",
      "name\n",
      "0.sq2.1 Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for n, m in sq4.named_modules():\n",
    "    ns = n.split('.')\n",
    "    print([n_.isdigit() for n_ in ns])\n",
    "    print('name')\n",
    "    print(n, m)   \n",
    "\n",
    "# for n, m in sq2._modules.items():\n",
    "#     print(n, m)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2000, 0.2000, 0.2000]),\n",
       " tensor([0.2000, 0.2000, 0.2000]),\n",
       " tensor([-0.3600]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "v2 = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "v3 = torch.tensor([5.], requires_grad=True)\n",
    "v4 = (v1.sum() + v2.sum()) / v3\n",
    "h = v3.register_hook(lambda grad: grad * 1.5)  # double the gradient\n",
    "\n",
    "v4.backward(torch.tensor([1.]))\n",
    "v1.grad, v2.grad, v3.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2000, 0.2000, 0.2000]),\n",
       " tensor([0.2000, 0.2000, 0.2000]),\n",
       " tensor([-1.4400]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.tensor([1., 4., 1.], requires_grad=True)\n",
    "v2 = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "v3 = torch.tensor([5.], requires_grad=True)\n",
    "v4 = (v1.sum() + v2.sum()) / v3\n",
    "h = v3.register_hook(lambda grad: grad * 3.0)  # double the gradient\n",
    "\n",
    "v4.backward(torch.tensor([1.]))\n",
    "v1.grad, v2.grad, v3.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WideResNet(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with Learning Rates and Decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8969, 1.8969, 1.8969, 1.8969, 1.8969],\n",
      "        [1.1014, 1.1014, 1.1014, 1.1014, 1.1014],\n",
      "        [1.5508, 1.5508, 1.5508, 1.5508, 1.5508],\n",
      "        [1.9652, 1.9652, 1.9652, 1.9652, 1.9652]], dtype=torch.float64)\n",
      "tensor([[1.8969, 1.8969, 1.8969, 1.8969, 1.8969],\n",
      "        [1.1014, 1.1014, 1.1014, 1.1014, 1.1014],\n",
      "        [1.5508, 1.5508, 1.5508, 1.5508, 1.5508],\n",
      "        [1.9652, 1.9652, 1.9652, 1.9652, 1.9652]], dtype=torch.float64)\n",
      "check_grad\n",
      "[[1.89687006 1.89687006 1.89687006 1.89687006 1.89687006]\n",
      " [1.10136331 1.10136331 1.10136331 1.10136331 1.10136331]\n",
      " [1.55079367 1.55079367 1.55079367 1.55079367 1.55079367]\n",
      " [1.96519422 1.96519422 1.96519422 1.96519422 1.96519422]]\n",
      "[[1.89687006 1.89687006 1.89687006 1.89687006 1.89687006]\n",
      " [1.10136331 1.10136331 1.10136331 1.10136331 1.10136331]\n",
      " [1.55079367 1.55079367 1.55079367 1.55079367 1.55079367]\n",
      " [1.96519422 1.96519422 1.96519422 1.96519422 1.96519422]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "np.set_printoptions(8, suppress=True)\n",
    "\n",
    "x_numpy = np.random.random((3, 4)).astype(np.double)\n",
    "x_torch = torch.tensor(x_numpy, requires_grad=True)\n",
    "x_torch2 = torch.tensor(x_numpy, requires_grad=True)\n",
    "\n",
    "w_numpy = np.random.random((4, 5)).astype(np.double)\n",
    "w_torch = torch.tensor(w_numpy, requires_grad=True)\n",
    "w_torch2 = torch.tensor(w_numpy, requires_grad=True)\n",
    "\n",
    "def log_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "w_torch.register_hook(log_grad)\n",
    "w_torch2.register_hook(log_grad)\n",
    "\n",
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0)\n",
    "sgd2 = torch.optim.SGD([w_torch2], lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "y_torch = torch.matmul(x_torch, w_torch)\n",
    "y_torch2 = torch.matmul(x_torch2, w_torch2)\n",
    "\n",
    "loss = y_torch.sum()\n",
    "loss2 = y_torch2.sum()\n",
    "\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss2.backward()\n",
    "\n",
    "sgd.step()\n",
    "sgd2.step()\n",
    "\n",
    "w_grad = w_torch.grad.data.numpy()\n",
    "w_grad2 = w_torch2.grad.data.numpy()\n",
    "\n",
    "print(\"check_grad\")\n",
    "print(w_grad)\n",
    "print(w_grad2 - weight_decay * w_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More fun with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Conv2d --------\n",
      "\n",
      "      Computing grads...\n",
      "         w-grad shape =  torch.Size([3, 3, 3, 3])\n",
      "         b-grad shape =  torch.Size([3])\n",
      "\n",
      "      Checking results...\n",
      "         Optimized weight - All close to 1: tensor(True)\n",
      "         Optimized Bias - All close to 1: tensor(False)\n",
      "\n",
      "------- Linear --------\n",
      "\n",
      "      Computing grads...\n",
      "         b-grad shape =  torch.Size([100])\n",
      "         w-grad shape =  torch.Size([100, 10])\n",
      "\n",
      "      Checking results...\n",
      "         Optimized weight - All close to 1: tensor(True)\n",
      "         Optimized Bias - All close to 1: tensor(False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -----------------\n",
    "# Helper function\n",
    "# -----------------\n",
    "def shape(t):\n",
    "    if isinstance(t, tuple):\n",
    "        return tuple(t_.shape if t_ is not None else None for t_ in t)\n",
    "    else:\n",
    "        return t.shape\n",
    "\n",
    "# -----------------\n",
    "# Grad hooks\n",
    "# -----------------\n",
    "\n",
    "# Zeros grad for weights\n",
    "def w_hook(grad):\n",
    "    print(' '*8, 'w-grad shape = ', shape(grad))\n",
    "    grad[:] = 0\n",
    "    return grad\n",
    "\n",
    "# No change for biases.\n",
    "def b_hook(grad):\n",
    "    print(' '*8, 'b-grad shape = ', shape(grad))\n",
    "    return grad\n",
    "\n",
    "# -----------------------\n",
    "# Test layers with biases\n",
    "# -----------------------\n",
    "\n",
    "# The following should confirm whether non-zero biases with non-zero gradient flows\n",
    "# yield changes to the the weights of the layer - indepent of those weights' grad flows.  \n",
    "\n",
    "layer1 = torch.nn.Conv2d(3, 3, 3)\n",
    "layer2 = torch.nn.Linear(10, 100)\n",
    "in1 = torch.rand(10, 3, 10, 10)\n",
    "in2 = torch.rand(10, 10, 10)\n",
    "\n",
    "for layer, input_ in [(layer1, in1), (layer2, in2)]:\n",
    "    \n",
    "    print('-------', layer.__class__.__name__, '--------\\n')\n",
    "    layer.weight.register_hook(w_hook)\n",
    "    layer.bias.register_hook(b_hook)\n",
    "\n",
    "    optim = torch.optim.SGD(layer.parameters(), lr=0.01)\n",
    "    \n",
    "    # Sets all weights and biases to 1.\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data[:] = 1\n",
    "        layer.bias.data[:] = 1\n",
    "\n",
    "    optim.zero_grad()\n",
    "    o = layer(input_)\n",
    "    loss = o.mean()\n",
    "    \n",
    "    print(' '*5, 'Computing grads...')\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    # See if weights and biases are still 1.\n",
    "    # This should only be the case for the weights\n",
    "    # as we zeroed their gradients.\n",
    "    print()\n",
    "    print(' '*5, 'Checking results...')\n",
    "    print(' '*8, 'Optimized weight - All close to 1:', (layer.weight == 1).all())\n",
    "    print(' '*8, 'Optimized Bias - All close to 1:', (layer.bias == 1).all())\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
